{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNfogcxQFhxf"
      },
      "source": [
        "# SemanticHearing Incremental Training\n",
        "\n",
        "This notebook demonstrates how to perform incremental training on the SemanticHearing model for binaural target sound extraction.\n",
        "\n",
        "## Overview\n",
        "- Load pre-trained model from the original paper\n",
        "- Prepare your additional training data\n",
        "- Fine-tune the model with new data\n",
        "- Evaluate performance improvements\n",
        "\n",
        "**Original Paper**: [Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables](https://dl.acm.org/doi/10.1145/3586183.3606779)\n",
        "\n",
        "**Repository**: https://github.com/sarahv03/SemanticHearing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV-42i_PFhxg"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's install the required dependencies and clone the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0m-8RejFhxg"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchaudio torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install librosa soundfile scipy matplotlib tqdm numpy pandas\n",
        "!pip install torchmetrics==0.10.0 seaborn ipykernel scaper\n",
        "!pip install transformers openl3 youtube_dl bs4 pyroomacoustics\n",
        "!pip install onnx onnxruntime torch_tb_profiler ffmpegio noisereduce\n",
        "!pip install tensorflow tensorflow-probability\n",
        "\n",
        "# Install additional packages for audio processing\n",
        "!pip install scaper thop==0.1.1.post2209072238\n",
        "!pip install python-sofa==0.2.0\n",
        "!pip install jams\n",
        "\n",
        "print(\"✅ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yq3TAj2Fhxg"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/sarahv03/SemanticHearing.git\n",
        "%cd SemanticHearing\n",
        "\n",
        "print(\"✅ Repository cloned successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No6RFIKqFhxg"
      },
      "source": [
        "## 2. Download Pre-trained Model and Dataset\n",
        "\n",
        "Download the pre-trained model checkpoint and the original dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryVG87drFhxh"
      },
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p experiments/dc_waveformer\n",
        "!mkdir -p data\n",
        "\n",
        "# Download pre-trained model checkpoint\n",
        "!wget -P experiments/dc_waveformer https://semantichearing.cs.washington.edu/39.pt\n",
        "\n",
        "print(\"✅ Pre-trained model downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-XEKOpCFhxh"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive and set up Google Cloud Storage access\n",
        "from google.colab import drive, auth\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Cloud Storage client\n",
        "client = storage.Client()\n",
        "\n",
        "# Your bucket and dataset paths\n",
        "BUCKET_NAME = \"misophones_training_dataset\"\n",
        "DATASET_PATH = \"FOAMS_dataset/FOAMS_processed_audio\"\n",
        "BINAURAL_DATASET_PATH = \"BinauralCuratedDataset\"  # Original dataset path in your bucket\n",
        "\n",
        "print(f\"✅ Google Cloud Storage client initialized\")\n",
        "print(f\"📦 Bucket: {BUCKET_NAME}\")\n",
        "print(f\"📁 Additional data path: {DATASET_PATH}\")\n",
        "print(f\"📁 Original dataset path: {BINAURAL_DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1Eu61JPFhxh"
      },
      "outputs": [],
      "source": [
        "# Download datasets from Google Cloud Storage\n",
        "def download_from_gcs(bucket_name, source_path, local_path):\n",
        "    \"\"\"Download files from Google Cloud Storage to local directory\"\"\"\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=source_path)\n",
        "\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "    downloaded_files = 0\n",
        "\n",
        "    for blob in blobs:\n",
        "        # Skip directories\n",
        "        if blob.name.endswith('/'):\n",
        "            continue\n",
        "\n",
        "        # Create local file path\n",
        "        local_file_path = os.path.join(local_path, blob.name.replace(source_path + '/', ''))\n",
        "        local_dir = os.path.dirname(local_file_path)\n",
        "        os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "        # Download file\n",
        "        blob.download_to_filename(local_file_path)\n",
        "        downloaded_files += 1\n",
        "\n",
        "        if downloaded_files % 100 == 0:\n",
        "            print(f\"Downloaded {downloaded_files} files...\")\n",
        "\n",
        "    print(f\"✅ Downloaded {downloaded_files} files from {source_path}\")\n",
        "    return downloaded_files\n",
        "\n",
        "\n",
        "\n",
        "# Download your additional FOAMS dataset\n",
        "print(\"📥 Downloading FOAMS dataset from Google Cloud Storage...\")\n",
        "download_from_gcs(BUCKET_NAME, DATASET_PATH, \"data/your_additional_data\")\n",
        "\n",
        "print(\"✅ All datasets downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S11W1n_Fhxh"
      },
      "source": [
        "## 3. Prepare Your Additional Data\n",
        "\n",
        "This section helps you prepare your additional training data. You'll need to:\n",
        "1. Upload your audio files\n",
        "2. Create proper data structure\n",
        "3. Generate labels for your sounds\n",
        "\n",
        "### Data Structure Requirements\n",
        "Your additional data should follow this structure:\n",
        "```\n",
        "your_data/\n",
        "├── train/\n",
        "│   ├── mixture/          # Mixed audio files\n",
        "│   ├── target/           # Target sound files\n",
        "│   └── labels/           # Label files (.jams format)\n",
        "├── val/\n",
        "│   ├── mixture/\n",
        "│   ├── target/\n",
        "│   └── labels/\n",
        "└── test/\n",
        "    ├── mixture/\n",
        "    ├── target/\n",
        "    └── labels/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aA50WArFhxh"
      },
      "outputs": [],
      "source": [
        "# Explore your FOAMS dataset structure\n",
        "def explore_dataset_structure(dataset_path):\n",
        "    \"\"\"Explore the structure of your FOAMS dataset\"\"\"\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    print(f\"🔍 Exploring dataset structure at: {dataset_path}\")\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"❌ Dataset path not found: {dataset_path}\")\n",
        "        return\n",
        "\n",
        "    # Walk through the directory structure\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        level = root.replace(dataset_path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "        # Show some files in each directory\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "# Explore the downloaded dataset\n",
        "explore_dataset_structure(\"data/your_additional_data\")\n",
        "\n",
        "print(\"\\n📋 Next steps:\")\n",
        "print(\"1. Review the dataset structure above\")\n",
        "print(\"2. Run the data preparation script to organize it for training\")\n",
        "print(\"3. The script will create the proper train/val/test splits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHQoSkbQFhxh"
      },
      "outputs": [],
      "source": [
        "# Create directory for your additional data\n",
        "!mkdir -p data/your_additional_data/{train,val,test}/{mixture,target,labels}\n",
        "\n",
        "print(\"📁 Created directory structure for your additional data.\")\n",
        "print(\"\\n📋 Next steps:\")\n",
        "print(\"1. Upload your audio files to the appropriate directories\")\n",
        "print(\"2. Create label files in .jams format\")\n",
        "print(\"3. Run the data preparation script below\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o6ehBmfFhxh"
      },
      "outputs": [],
      "source": [
        "# Helper function to create JAMS labels\n",
        "def create_jams_label(audio_file, target_sound_class, start_time=0.0, end_time=None):\n",
        "    \"\"\"\n",
        "    Create a JAMS label file for your audio data.\n",
        "\n",
        "    Args:\n",
        "        audio_file: Path to audio file\n",
        "        target_sound_class: Class of the target sound (e.g., 'speech', 'music', 'bird')\n",
        "        start_time: Start time of target sound in seconds\n",
        "        end_time: End time of target sound in seconds (None for full duration)\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    import jams\n",
        "    \n",
        "    # Load audio to get duration\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    duration = len(y) / sr\n",
        "\n",
        "    if end_time is None:\n",
        "        end_time = duration\n",
        "\n",
        "    # Create JAMS annotation\n",
        "    jam = jams.JAMS()\n",
        "    jam.file_metadata.duration = duration\n",
        "\n",
        "    # Create annotation for target sound\n",
        "    ann = jams.Annotation(namespace='tag_open')\n",
        "    ann.append(time=start_time, duration=end_time-start_time, value=target_sound_class, confidence=1.0)\n",
        "\n",
        "    jam.annotations.append(ann)\n",
        "\n",
        "    return jam\n",
        "\n",
        "# Updated data preparation function for FOAMS dataset\n",
        "def prepare_foams_data(data_dir, target_class=\"speech\"):\n",
        "    \"\"\"\n",
        "    Prepare your FOAMS dataset for training by creating proper train/val/test splits.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing your FOAMS audio files\n",
        "        target_class: Class name for your target sounds (default: \"speech\")\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "\n",
        "    data_path = Path(data_dir)\n",
        "\n",
        "    # Find all audio files recursively\n",
        "    audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']\n",
        "    all_audio_files = []\n",
        "\n",
        "    for ext in audio_extensions:\n",
        "        all_audio_files.extend(data_path.rglob(ext))\n",
        "\n",
        "    print(f\"Found {len(all_audio_files)} audio files\")\n",
        "\n",
        "    # Shuffle and split the data\n",
        "    random.shuffle(all_audio_files)\n",
        "\n",
        "    # Split: 80% train, 10% val, 10% test\n",
        "    train_size = int(0.8 * len(all_audio_files))\n",
        "    val_size = int(0.1 * len(all_audio_files))\n",
        "\n",
        "    train_files = all_audio_files[:train_size]\n",
        "    val_files = all_audio_files[train_size:train_size + val_size]\n",
        "    test_files = all_audio_files[train_size + val_size:]\n",
        "\n",
        "    print(f\"Split: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
        "\n",
        "    # Process each split\n",
        "    for split_name, files in [('train', train_files), ('val', val_files), ('test', test_files)]:\n",
        "        print(f\"\\nProcessing {split_name} split...\")\n",
        "\n",
        "        # Create directories\n",
        "        split_dir = data_path / split_name\n",
        "        (split_dir / 'mixture').mkdir(parents=True, exist_ok=True)\n",
        "        (split_dir / 'target').mkdir(parents=True, exist_ok=True)\n",
        "        (split_dir / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for i, audio_file in enumerate(files):\n",
        "            # Create new filename with index\n",
        "            new_name = f\"{i:06d}{audio_file.suffix}\"\n",
        "\n",
        "            # Copy to mixture directory\n",
        "            mixture_file = split_dir / 'mixture' / new_name\n",
        "            shutil.copy2(audio_file, mixture_file)\n",
        "\n",
        "            # Copy to target directory (same file for now)\n",
        "            target_file = split_dir / 'target' / new_name\n",
        "            shutil.copy2(audio_file, target_file)\n",
        "\n",
        "            # Create JAMS label\n",
        "            label_file = split_dir / 'labels' / f\"{i:06d}.jams\"\n",
        "            jam = create_jams_label(mixture_file, target_class)\n",
        "            jam.save(str(label_file))\n",
        "\n",
        "        print(f\"✅ Processed {len(files)} files in {split_name} split\")\n",
        "\n",
        "    print(\"\\n🎉 FOAMS dataset preparation completed!\")\n",
        "    print(\"Your data is now organized in the format expected by the training pipeline.\")\n",
        "\n",
        "# Run the data preparation\n",
        "print(\"🚀 Preparing FOAMS dataset for training...\")\n",
        "prepare_foams_data(\"data/your_additional_data\", target_class=\"speech\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DJunLpRFhxh"
      },
      "outputs": [],
      "source": [
        "# Convert spreadsheet labels to JAMS format for incremental training\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import jams\n",
        "\n",
        "def convert_spreadsheet_to_jams(spreadsheet_path, audio_dir, output_dir, target_class=\"speech\"):\n",
        "    \"\"\"\n",
        "    Convert spreadsheet with labels to JAMS format for SemanticHearing training.\n",
        "\n",
        "    Required spreadsheet columns:\n",
        "    - filename: Name of the audio file\n",
        "    - start_time: Start time of the target sound (seconds)\n",
        "    - end_time: End time of the target sound (seconds)\n",
        "    - label: Target sound class (must be in predefined list)\n",
        "\n",
        "    Predefined sound classes:\n",
        "    \"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\",\n",
        "    \"cock_a_doodle_doo\", \"cricket\", \"computer_typing\",\n",
        "    \"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\",\n",
        "    \"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\",\n",
        "    \"thunderstorm\", \"toilet_flush\"\n",
        "\n",
        "    Args:\n",
        "        spreadsheet_path: Path to your CSV/Excel file\n",
        "        audio_dir: Directory containing the audio files\n",
        "        output_dir: Directory to save organized data\n",
        "        target_class: Default class name if not specified in spreadsheet\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the spreadsheet\n",
        "    if spreadsheet_path.endswith('.csv'):\n",
        "        df = pd.read_csv(spreadsheet_path)\n",
        "    elif spreadsheet_path.endswith(('.xlsx', '.xls')):\n",
        "        df = pd.read_excel(spreadsheet_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV or Excel files.\")\n",
        "\n",
        "    print(f\"📊 Loaded {len(df)} entries from spreadsheet\")\n",
        "    print(f\"📁 Audio directory: {audio_dir}\")\n",
        "    print(f\"📁 Output directory: {output_dir}\")\n",
        "\n",
        "    # Create output directories\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for subdir in ['mixture', 'target', 'labels']:\n",
        "            os.makedirs(os.path.join(output_dir, split, subdir), exist_ok=True)\n",
        "\n",
        "    # Split data: 80% train, 10% val, 10% test\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "    train_size = int(0.8 * len(df))\n",
        "    val_size = int(0.1 * len(df))\n",
        "\n",
        "    splits = {\n",
        "        'train': df[:train_size],\n",
        "        'val': df[train_size:train_size + val_size],\n",
        "        'test': df[train_size + val_size:]\n",
        "    }\n",
        "\n",
        "    print(f\"📊 Data split: {len(splits['train'])} train, {len(splits['val'])} val, {len(splits['test'])} test\")\n",
        "\n",
        "    # Process each split\n",
        "    for split_name, split_df in splits.items():\n",
        "        print(f\"\\n🔄 Processing {split_name} split...\")\n",
        "\n",
        "        for idx, row in split_df.iterrows():\n",
        "            try:\n",
        "                # Get file info\n",
        "                filename = row['filename']\n",
        "                start_time = float(row['start_time'])\n",
        "                end_time = float(row['end_time'])\n",
        "                label = row.get('label', target_class)\n",
        "\n",
        "                # Validate label is in predefined list\n",
        "                predefined_labels = [\n",
        "                    \"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\",\n",
        "                    \"cock_a_doodle_doo\", \"cricket\", \"computer_typing\",\n",
        "                    \"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\",\n",
        "                    \"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\",\n",
        "                    \"thunderstorm\", \"toilet_flush\"\n",
        "                ]\n",
        "\n",
        "                if label not in predefined_labels:\n",
        "                    print(f\"⚠️  Warning: '{label}' not in predefined list. Using '{target_class}' instead.\")\n",
        "                    label = target_class\n",
        "\n",
        "                # Find the audio file\n",
        "                audio_file = None\n",
        "                for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                    potential_file = os.path.join(audio_dir, filename + ext)\n",
        "                    if os.path.exists(potential_file):\n",
        "                        audio_file = potential_file\n",
        "                        break\n",
        "\n",
        "                if not audio_file:\n",
        "                    print(f\"⚠️  Audio file not found: {filename}\")\n",
        "                    continue\n",
        "\n",
        "                # Load audio to get duration\n",
        "                y, sr = librosa.load(audio_file, sr=None)\n",
        "                duration = len(y) / sr\n",
        "\n",
        "                # Create new filename with index\n",
        "                new_filename = f\"{idx:06d}{os.path.splitext(audio_file)[1]}\"\n",
        "\n",
        "                # Copy to mixture directory\n",
        "                mixture_file = os.path.join(output_dir, split_name, 'mixture', new_filename)\n",
        "                import shutil\n",
        "                shutil.copy2(audio_file, mixture_file)\n",
        "\n",
        "                # Extract target segment and save to target directory\n",
        "                target_file = os.path.join(output_dir, split_name, 'target', new_filename)\n",
        "                start_sample = int(start_time * sr)\n",
        "                end_sample = int(end_time * sr)\n",
        "                target_audio = y[start_sample:end_sample]\n",
        "\n",
        "                # Ensure stereo\n",
        "                if target_audio.ndim == 1:\n",
        "                    target_audio = np.stack([target_audio, target_audio], axis=0)\n",
        "                elif target_audio.shape[0] == 1:\n",
        "                    target_audio = np.repeat(target_audio, 2, axis=0)\n",
        "\n",
        "                import soundfile as sf\n",
        "                sf.write(target_file, target_audio.T, sr)\n",
        "\n",
        "                # Create JAMS label file\n",
        "                jam = jams.JAMS()\n",
        "                jam.file_metadata.duration = duration\n",
        "\n",
        "                # Create annotation for the target sound\n",
        "                target_ann = jams.Annotation(namespace='tag_open')\n",
        "                target_ann.append(\n",
        "                    time=start_time,\n",
        "                    duration=end_time - start_time,\n",
        "                    value=label,\n",
        "                    confidence=1.0\n",
        "                )\n",
        "                jam.annotations.append(target_ann)\n",
        "\n",
        "                # Save JAMS file\n",
        "                jams_file = os.path.join(output_dir, split_name, 'labels', f\"{idx:06d}.jams\")\n",
        "                jam.save(jams_file)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {filename}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ Processed {len(split_df)} files in {split_name} split\")\n",
        "\n",
        "    print(f\"\\n🎉 Conversion completed!\")\n",
        "    print(f\"📁 Organized data saved to: {output_dir}\")\n",
        "    print(f\"📊 Ready for incremental training!\")\n",
        "\n",
        "# Example usage - update these paths for your data\n",
        "print(\"📝 Spreadsheet to JAMS conversion script ready!\")\n",
        "print(\"\\nTo use this script:\")\n",
        "print(\"1. Prepare your spreadsheet with columns: filename, start_time, end_time, label\")\n",
        "print(\"2. Update the paths below:\")\n",
        "print(\"3. Run: convert_spreadsheet_to_jams('your_labels.csv', 'audio_directory', 'output_directory')\")\n",
        "print(\"\\nExample:\")\n",
        "print(\"convert_spreadsheet_to_jams('labels.csv', 'data/your_additional_data', 'data/foams_organized', 'chewing')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cddnPr7Fhxi"
      },
      "source": [
        "## Spreadsheet Format Requirements\n",
        "\n",
        "Based on the SemanticHearing codebase, here's the correct format:\n",
        "\n",
        "### Required Columns:\n",
        "| Column Name | Description | Example |\n",
        "|-------------|-------------|---------|\n",
        "| `filename` | Audio file name (without extension) | `audio_001` |\n",
        "| `start_time` | Start time of target sound (seconds) | `2.3` |\n",
        "| `end_time` | End time of target sound (seconds) | `5.4` |\n",
        "| `label` | Target sound class (must be in predefined list) | `speech` |\n",
        "\n",
        "### Background vs Foreground:\n",
        "- **Foreground (target)**: Sounds you want to extract (must be in the 20 predefined classes)\n",
        "- **Background (non-target)**: Everything else that should be filtered out\n",
        "- **The model determines foreground/background by the label type**, not separate columns\n",
        "\n",
        "### Predefined Sound Classes (from the codebase):\n",
        "```\n",
        "\"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\",\n",
        "\"cock_a_doodle_doo\", \"cricket\", \"computer_typing\",\n",
        "\"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\",\n",
        "\"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\",\n",
        "\"thunderstorm\", \"toilet_flush\"\n",
        "```\n",
        "\n",
        "### Example Spreadsheet:\n",
        "```csv\n",
        "filename,start_time,end_time,label\n",
        "audio_001,2.3,5.4,speech\n",
        "audio_002,0.5,3.2,music\n",
        "audio_003,1.1,4.7,speech\n",
        "audio_004,2.0,4.0,computer_typing\n",
        "```\n",
        "\n",
        "### For Misophonia (Chewing Sounds):\n",
        "Since \"chewing\" is not in the predefined list, you have two options:\n",
        "1. **Use \"speech\"** as the closest category\n",
        "2. **Add \"chewing\" to the model's label list** (requires code modification)\n",
        "\n",
        "### Supported File Formats:\n",
        "- **CSV files** (`.csv`)\n",
        "- **Excel files** (`.xlsx`, `.xls`)\n",
        "\n",
        "### What the Script Does:\n",
        "1. **Reads your spreadsheet** with timing information\n",
        "2. **Finds corresponding audio files** (supports .wav, .mp3, .flac, .m4a)\n",
        "3. **Extracts target segments** based on start/end times\n",
        "4. **Creates proper directory structure** for training\n",
        "5. **Generates JAMS label files** in the required format\n",
        "6. **Splits data** into train/val/test (80/10/10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TVYd-7KFhxi"
      },
      "outputs": [],
      "source": [
        "# Create example spreadsheet with background noise information\n",
        "import pandas as pd\n",
        "\n",
        "def create_example_spreadsheet():\n",
        "    \"\"\"Create an example spreadsheet showing the correct format for SemanticHearing\"\"\"\n",
        "\n",
        "    # Example data using predefined sound classes\n",
        "    data = {\n",
        "        'filename': ['audio_001', 'audio_002', 'audio_003', 'audio_004', 'audio_005'],\n",
        "        'start_time': [2.3, 0.5, 1.1, 3.2, 0.8],\n",
        "        'end_time': [5.4, 3.2, 4.7, 6.1, 2.9],\n",
        "        'label': ['speech', 'speech', 'speech', 'computer_typing', 'speech']\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save as CSV\n",
        "    df.to_csv('example_labels_with_background.csv', index=False)\n",
        "\n",
        "    print(\"📊 Example spreadsheet created: example_labels_with_background.csv\")\n",
        "    print(\"\\n📋 This shows the recommended format with background noise information:\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create the example\n",
        "example_df = create_example_spreadsheet()\n",
        "\n",
        "print(\"\\n💡 Key points about the correct format:\")\n",
        "print(\"• Use only predefined sound classes from the SemanticHearing model\")\n",
        "print(\"• Foreground vs background is determined by the label type\")\n",
        "print(\"• For misophonia (chewing), use 'speech' as the closest category\")\n",
        "print(\"• The model will learn to extract target sounds and filter everything else\")\n",
        "print(\"\\n🎯 This format helps the model:\")\n",
        "print(\"• Learn to distinguish between target and non-target sounds\")\n",
        "print(\"• Focus on the specific sounds you want to extract\")\n",
        "print(\"• Work with the existing model architecture\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pDWRamDFhxi"
      },
      "source": [
        "## Why Include Background Noise Information?\n",
        "\n",
        "### **SemanticHearing Model Architecture:**\n",
        "The SemanticHearing model is specifically designed to:\n",
        "- **Extract target sounds** (foreground) from mixed audio\n",
        "- **Suppress background noise** while preserving spatial cues\n",
        "- **Learn the distinction** between what to keep vs. what to filter\n",
        "\n",
        "### **Benefits of Background Noise Labels:**\n",
        "\n",
        "1. **Better Target Extraction** 🎯\n",
        "   - Model learns to focus on specific sounds (chewing)\n",
        "   - Reduces false positives from background noise\n",
        "   - Improves precision in noisy environments\n",
        "\n",
        "2. **Improved Generalization** 🌍\n",
        "   - Trains on diverse acoustic environments\n",
        "   - Learns to handle different background types\n",
        "   - Better performance in real-world scenarios\n",
        "\n",
        "3. **Spatial Audio Preservation** 🎧\n",
        "   - Maintains binaural cues for target sounds\n",
        "   - Filters background while preserving directionality\n",
        "   - Essential for misophonia applications\n",
        "\n",
        "4. **Training Efficiency** ⚡\n",
        "   - Model learns faster with explicit background labels\n",
        "   - Better convergence during incremental training\n",
        "   - More robust to different noise levels\n",
        "\n",
        "### **Background Noise Types to Include:**\n",
        "- **Traffic** (cars, trucks, motorcycles)\n",
        "- **Restaurant** (chatter, dishes, ambient noise)\n",
        "- **Office** (keyboard typing, air conditioning, conversations)\n",
        "- **Music** (background music, radio)\n",
        "- **Nature** (wind, rain, birds)\n",
        "- **Home** (appliances, TV, family sounds)\n",
        "\n",
        "### **Intensity Guidelines:**\n",
        "- **0.0-0.3**: Quiet background (library, quiet room)\n",
        "- **0.3-0.6**: Moderate background (cafe, office)\n",
        "- **0.6-0.8**: Loud background (restaurant, traffic)\n",
        "- **0.8-1.0**: Very loud background (construction, loud music)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZGAve3QFhxi"
      },
      "outputs": [],
      "source": [
        "# Updated conversion function with C1/C2 prefix support and 51 labels\n",
        "def convert_spreadsheet_to_jams_updated(spreadsheet_path, audio_dir, output_dir, target_class=\"speech\"):\n",
        "    \"\"\"\n",
        "    Convert spreadsheet data to JAMS format for SemanticHearing training.\n",
        "    Supports C1/C2 prefix system and all 51 sound classes.\n",
        "    \n",
        "    Args:\n",
        "        spreadsheet_path: Path to CSV/Excel file with columns: filename, start_time, end_time, label\n",
        "        audio_dir: Directory containing the audio files\n",
        "        output_dir: Directory to save organized data\n",
        "        target_class: Fallback class for unknown labels\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    import librosa\n",
        "    import jams\n",
        "    import numpy as np\n",
        "    \n",
        "    # Load spreadsheet\n",
        "    if spreadsheet_path.endswith('.csv'):\n",
        "        df = pd.read_csv(spreadsheet_path)\n",
        "    elif spreadsheet_path.endswith(('.xlsx', '.xls')):\n",
        "        df = pd.read_excel(spreadsheet_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV or Excel files.\")\n",
        "    \n",
        "    print(f\"📊 Loaded {len(df)} entries from spreadsheet\")\n",
        "    print(f\"📁 Audio directory: {audio_dir}\")\n",
        "    print(f\"📁 Output directory: {output_dir}\")\n",
        "    \n",
        "    # Create output directories\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for subdir in ['mixture', 'target', 'labels']:\n",
        "            os.makedirs(os.path.join(output_dir, split, subdir), exist_ok=True)\n",
        "    \n",
        "    # Split data: 80% train, 10% val, 10% test\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "    train_size = int(0.8 * len(df))\n",
        "    val_size = int(0.1 * len(df))\n",
        "    \n",
        "    splits = {\n",
        "        'train': df[:train_size],\n",
        "        'val': df[train_size:train_size + val_size],\n",
        "        'test': df[train_size + val_size:]\n",
        "    }\n",
        "    \n",
        "    print(f\"📊 Data split: {len(splits['train'])} train, {len(splits['val'])} val, {len(splits['test'])} test\")\n",
        "    \n",
        "    # All labels that the model can recognize (51 total)\n",
        "    predefined_labels = [\n",
        "        # Original 20 labels\n",
        "        \"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\", \n",
        "        \"cock_a_doodle_doo\", \"cricket\", \"computer_typing\", \n",
        "        \"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\", \n",
        "        \"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\", \n",
        "        \"thunderstorm\", \"toilet_flush\",\n",
        "        # New labels from FOAMS dataset\n",
        "        \"artifact\", \"basketball_dribbling\", \"beeping\", \"chewing_gum\",\n",
        "        \"clearing_throat\", \"clicking\", \"coffee_shop\", \"coughing\", \n",
        "        \"drumming\", \"exhaling\", \"flipping_newspaper_pages\", \"footsteps\",\n",
        "        \"hairdryer\", \"harp\", \"human_breathing\", \"knife_cutting\", \n",
        "        \"laughing\", \"lip_smacks\", \"mouth_sounds_other\", \"plastic_crumpling\",\n",
        "        \"recording_artifact\", \"sliding_ceramic\", \"slurping\", \"squishing\",\n",
        "        \"swallowing\", \"talking\", \"tearing_paper\", \"typing\",\n",
        "        \"water_drops\", \"whimpering\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"🎯 Supporting {len(predefined_labels)} sound classes\")\n",
        "    \n",
        "    # Process each split\n",
        "    for split_name, split_df in splits.items():\n",
        "        print(f\"\\n🔄 Processing {split_name} split...\")\n",
        "        \n",
        "        for idx, row in split_df.iterrows():\n",
        "            try:\n",
        "                # Get file info\n",
        "                filename = row['filename']\n",
        "                start_time = float(row['start_time'])\n",
        "                end_time = float(row['end_time'])\n",
        "                label = row.get('label', target_class)\n",
        "                \n",
        "                # Handle C1/C2 prefix system\n",
        "                if label.startswith('C1-'):\n",
        "                    # C1 = target sound (foreground)\n",
        "                    clean_label = label[3:]  # Remove 'C1-' prefix\n",
        "                    is_target = True\n",
        "                elif label.startswith('C2-'):\n",
        "                    # C2 = background sound\n",
        "                    clean_label = label[3:]  # Remove 'C2-' prefix\n",
        "                    is_target = False\n",
        "                else:\n",
        "                    # No prefix - assume target sound\n",
        "                    clean_label = label\n",
        "                    is_target = True\n",
        "                \n",
        "                if clean_label not in predefined_labels:\n",
        "                    print(f\"⚠️  Warning: '{clean_label}' not in predefined list. Using 'speech' instead.\")\n",
        "                    clean_label = 'speech'\n",
        "                \n",
        "                # Use the clean label for processing\n",
        "                label = clean_label\n",
        "                \n",
        "                # Find the audio file\n",
        "                audio_file = None\n",
        "                for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                    potential_file = os.path.join(audio_dir, filename + ext)\n",
        "                    if os.path.exists(potential_file):\n",
        "                        audio_file = potential_file\n",
        "                        break\n",
        "                \n",
        "                if not audio_file:\n",
        "                    print(f\"⚠️  Audio file not found: {filename}\")\n",
        "                    continue\n",
        "                \n",
        "                # Load audio to get duration\n",
        "                y, sr = librosa.load(audio_file, sr=None)\n",
        "                duration = len(y) / sr\n",
        "                \n",
        "                # Create new filename with index\n",
        "                new_filename = f\"{idx:06d}{os.path.splitext(audio_file)[1]}\"\n",
        "                \n",
        "                # Copy to mixture directory\n",
        "                mixture_file = os.path.join(output_dir, split_name, 'mixture', new_filename)\n",
        "                import shutil\n",
        "                shutil.copy2(audio_file, mixture_file)\n",
        "                \n",
        "                # Extract target segment and save to target directory\n",
        "                target_file = os.path.join(output_dir, split_name, 'target', new_filename)\n",
        "                start_sample = int(start_time * sr)\n",
        "                end_sample = int(end_time * sr)\n",
        "                target_audio = y[start_sample:end_sample]\n",
        "                \n",
        "                # Ensure stereo\n",
        "                if target_audio.ndim == 1:\n",
        "                    target_audio = np.stack([target_audio, target_audio], axis=0)\n",
        "                elif target_audio.shape[0] == 1:\n",
        "                    target_audio = np.repeat(target_audio, 2, axis=0)\n",
        "                \n",
        "                import soundfile as sf\n",
        "                sf.write(target_file, target_audio.T, sr)\n",
        "                \n",
        "                # Create JAMS label file\n",
        "                jam = jams.JAMS()\n",
        "                jam.file_metadata.duration = duration\n",
        "                jam.file_metadata.identifiers = {'filename': filename}\n",
        "                \n",
        "                # Create annotation for the target sound\n",
        "                target_ann = jams.Annotation(namespace='tag_open')\n",
        "                target_ann.append(\n",
        "                    time=start_time,\n",
        "                    duration=end_time - start_time,\n",
        "                    value=label,\n",
        "                    confidence=1.0\n",
        "                )\n",
        "                \n",
        "                # Add metadata about target/background\n",
        "                target_ann.annotation_metadata.data_source = 'foams_dataset'\n",
        "                target_ann.annotation_metadata.annotation_tools = 'manual'\n",
        "                target_ann.annotation_metadata.annotation_rules = f'target_sound: {is_target}'\n",
        "                \n",
        "                jam.annotations.append(target_ann)\n",
        "                \n",
        "                # Save JAMS file\n",
        "                jams_file = os.path.join(output_dir, split_name, 'labels', f\"{idx:06d}.jams\")\n",
        "                jam.save(jams_file)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {filename}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"✅ Processed {len(split_df)} files in {split_name} split\")\n",
        "    \n",
        "    print(f\"\\n🎉 Conversion completed!\")\n",
        "    print(f\"📁 Organized data saved to: {output_dir}\")\n",
        "    print(f\"📊 Ready for incremental training with 51 sound classes!\")\n",
        "\n",
        "print(\"✅ Updated conversion function ready!\")\n",
        "print(\"🎯 Now supports C1/C2 prefix system and all 51 sound classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Convert your spreadsheet labels to JAMS format\n",
        "# Update these paths to match your data\n",
        "\n",
        "# Path to your spreadsheet (upload to Colab or place in Google Drive)\n",
        "spreadsheet_path = \"/foams_annotation .csv\"  # or \"labels.xlsx\"\n",
        "\n",
        "# Path to your audio files (downloaded from Google Cloud Storage)\n",
        "audio_directory = \"gs://misophones_training_dataset/FOAMS_dataset/FOAMS_processed_audio\"\n",
        "\n",
        "# Output directory for organized data\n",
        "output_directory = \"data/foams_organized\"\n",
        "\n",
        "# Target sound class (e.g., \"chewing\", \"speech\", \"music\")\n",
        "target_class = \"chewing\"\n",
        "\n",
        "print(\"📋 Ready to convert your spreadsheet labels!\")\n",
        "print(f\"📊 Spreadsheet: {spreadsheet_path}\")\n",
        "print(f\"🎵 Audio directory: {audio_directory}\")\n",
        "print(f\"📁 Output directory: {output_directory}\")\n",
        "print(f\"🎯 Target class: {target_class}\")\n",
        "\n",
        "# Uncomment the line below to run the conversion\n",
        "# convert_spreadsheet_to_jams(spreadsheet_path, audio_directory, output_directory, target_class)\n",
        "\n",
        "print(\"\\n💡 To run the conversion:\")\n",
        "print(\"1. Upload your spreadsheet to Colab\")\n",
        "print(\"2. Update the paths above\")\n",
        "print(\"3. Uncomment the convert_spreadsheet_to_jams() line\")\n",
        "print(\"4. Run this cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6alaePzSgsu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPhHU7wCFhxi"
      },
      "outputs": [],
      "source": [
        "# Data preparation script for your additional data\n",
        "import os\n",
        "import json\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import jams\n",
        "\n",
        "def create_jams_label(audio_file, target_sound_class, start_time=0.0, end_time=None):\n",
        "    \"\"\"\n",
        "    Create a JAMS label file for your audio data.\n",
        "\n",
        "    Args:\n",
        "        audio_file: Path to audio file\n",
        "        target_sound_class: Class of the target sound (e.g., 'speech', 'music', 'bird')\n",
        "        start_time: Start time of target sound in seconds\n",
        "        end_time: End time of target sound in seconds (None for full duration)\n",
        "    \"\"\"\n",
        "    # Load audio to get duration\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    duration = len(y) / sr\n",
        "\n",
        "    if end_time is None:\n",
        "        end_time = duration\n",
        "\n",
        "    # Create JAMS annotation\n",
        "    jam = jams.JAMS()\n",
        "    jam.file_metadata.duration = duration\n",
        "\n",
        "    # Create annotation for target sound\n",
        "    ann = jams.Annotation(namespace='tag_open')\n",
        "    ann.append(time=start_time, duration=end_time-start_time, value=target_sound_class, confidence=1.0)\n",
        "\n",
        "    jam.annotations.append(ann)\n",
        "\n",
        "    return jam\n",
        "\n",
        "def prepare_your_data(data_dir, target_class):\n",
        "    \"\"\"\n",
        "    Prepare your data by creating JAMS labels and organizing files.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing your audio files\n",
        "        target_class: Class name for your target sounds\n",
        "    \"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "\n",
        "    # Process each split\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_dir = data_path / split\n",
        "        if not split_dir.exists():\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {split} split...\")\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        (split_dir / 'mixture').mkdir(exist_ok=True)\n",
        "        (split_dir / 'target').mkdir(exist_ok=True)\n",
        "        (split_dir / 'labels').mkdir(exist_ok=True)\n",
        "\n",
        "        # Process audio files\n",
        "        audio_files = list(split_dir.glob('*.wav')) + list(split_dir.glob('*.mp3')) + list(split_dir.glob('*.flac'))\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            # Move to mixture directory\n",
        "            mixture_file = split_dir / 'mixture' / audio_file.name\n",
        "            if not mixture_file.exists():\n",
        "                audio_file.rename(mixture_file)\n",
        "\n",
        "            # Create target file (copy for now - you may want to extract specific parts)\n",
        "            target_file = split_dir / 'target' / audio_file.name\n",
        "            if not target_file.exists():\n",
        "                import shutil\n",
        "                shutil.copy2(mixture_file, target_file)\n",
        "\n",
        "            # Create JAMS label\n",
        "            label_file = split_dir / 'labels' / (audio_file.stem + '.jams')\n",
        "            if not label_file.exists():\n",
        "                jam = create_jams_label(mixture_file, target_class)\n",
        "                jam.save(str(label_file))\n",
        "\n",
        "        print(f\"✅ Processed {len(audio_files)} files in {split} split\")\n",
        "\n",
        "# Example usage - modify the paths and target class as needed\n",
        "print(\"📝 Data preparation script ready!\")\n",
        "print(\"\\nTo use this script:\")\n",
        "print(\"1. Upload your audio files to data/your_additional_data/train/, data/your_additional_data/val/, etc.\")\n",
        "print(\"2. Run: prepare_your_data('data/your_additional_data', 'your_target_class')\")\n",
        "print(\"3. Replace 'your_target_class' with the actual class name (e.g., 'speech', 'music', 'bird')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzVGVZb3Fhxi"
      },
      "outputs": [],
      "source": [
        "# Create training configuration for organized spreadsheet data\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration for data organized from spreadsheet labels\n",
        "organized_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": 8,\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/foams_organized/train/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/foams_organized/val/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/foams_organized/test/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,\n",
        "        \"weight_decay\": 1e-5\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"organized_spreadsheet_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/organized_training', exist_ok=True)\n",
        "with open('experiments/organized_training/config.json', 'w') as f:\n",
        "    json.dump(organized_config, f, indent=4)\n",
        "\n",
        "print(\"✅ Configuration for organized spreadsheet data created!\")\n",
        "print(\"📁 Saved to: experiments/organized_training/config.json\")\n",
        "print(\"\\n🎯 This configuration uses data organized from your spreadsheet labels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP2dvNK5Fhxi"
      },
      "source": [
        "## 📊 **Step-by-Step: Upload and Process Your Spreadsheet Data**\n",
        "\n",
        "Follow these steps to upload your spreadsheet and start training:\n",
        "\n",
        "### **Step 1: Upload Your Spreadsheet**\n",
        "1. **Click the folder icon** 📁 in the left sidebar of Colab\n",
        "2. **Click \"Upload to session storage\"** \n",
        "3. **Drag and drop** your CSV/Excel file, or click to browse\n",
        "4. **Wait for upload** to complete\n",
        "\n",
        "### **Step 2: Update the File Paths**\n",
        "Replace the paths below with your actual file names:\n",
        "\n",
        "```python\n",
        "# Update these paths to match your uploaded files\n",
        "SPREADSHEET_PATH = \"your_spreadsheet.csv\"  # Change this to your file name\n",
        "AUDIO_DIR = \"/content/your_audio_files\"    # Path to your audio files\n",
        "OUTPUT_DIR = \"/content/foams_organized\"    # Where to save processed data\n",
        "```\n",
        "\n",
        "### **Step 3: Run the Conversion**\n",
        "Execute the cell below to process your data:\n",
        "\n",
        "```python\n",
        "# Process your spreadsheet data\n",
        "convert_spreadsheet_to_jams_updated(\n",
        "    spreadsheet_path=SPREADSHEET_PATH,\n",
        "    audio_dir=AUDIO_DIR, \n",
        "    output_dir=OUTPUT_DIR\n",
        ")\n",
        "```\n",
        "\n",
        "### **Step 4: Start Training**\n",
        "Once conversion is complete, run the training cell to start fine-tuning!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 **READY TO USE: Process Your Spreadsheet Data**\n",
        "\n",
        "# Step 1: Update these paths to match your uploaded files\n",
        "SPREADSHEET_PATH = \"your_spreadsheet.csv\"  # ← Change this to your actual file name\n",
        "AUDIO_DIR = \"/content/your_audio_files\"    # ← Path to your audio files  \n",
        "OUTPUT_DIR = \"/content/foams_organized\"    # ← Where to save processed data\n",
        "\n",
        "print(\"📋 Configuration:\")\n",
        "print(f\"   📊 Spreadsheet: {SPREADSHEET_PATH}\")\n",
        "print(f\"   🎵 Audio files: {AUDIO_DIR}\")\n",
        "print(f\"   📁 Output: {OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "# Step 2: Check if files exist\n",
        "import os\n",
        "if os.path.exists(SPREADSHEET_PATH):\n",
        "    print(\"✅ Spreadsheet found!\")\n",
        "else:\n",
        "    print(\"❌ Spreadsheet not found. Please upload your file first.\")\n",
        "    print(\"   💡 Click the folder icon 📁 in the left sidebar to upload\")\n",
        "\n",
        "if os.path.exists(AUDIO_DIR):\n",
        "    print(\"✅ Audio directory found!\")\n",
        "else:\n",
        "    print(\"❌ Audio directory not found. Please update AUDIO_DIR path.\")\n",
        "    print(\"   💡 Make sure your audio files are accessible\")\n",
        "\n",
        "print()\n",
        "print(\"🎯 Ready to process your data!\")\n",
        "print(\"   Run the next cell to start conversion...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎬 **RUN THIS: Convert Your Spreadsheet to Training Data**\n",
        "\n",
        "# Process your spreadsheet data with C1/C2 prefix support\n",
        "print(\"🚀 Starting data conversion...\")\n",
        "print(\"📊 Processing your spreadsheet with 51 sound classes\")\n",
        "print(\"🎯 C1 = Target sounds, C2 = Background sounds\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Convert your spreadsheet to JAMS format\n",
        "    convert_spreadsheet_to_jams_updated(\n",
        "        spreadsheet_path=SPREADSHEET_PATH,\n",
        "        audio_dir=AUDIO_DIR,\n",
        "        output_dir=OUTPUT_DIR\n",
        "    )\n",
        "    \n",
        "    print()\n",
        "    print(\"🎉 SUCCESS! Your data is ready for training!\")\n",
        "    print(\"📁 Check the output directory for organized data:\")\n",
        "    print(f\"   • {OUTPUT_DIR}/train/ (80% of your data)\")\n",
        "    print(f\"   • {OUTPUT_DIR}/val/ (10% of your data)\")  \n",
        "    print(f\"   • {OUTPUT_DIR}/test/ (10% of your data)\")\n",
        "    print()\n",
        "    print(\"🚀 Ready to start incremental training!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during conversion: {str(e)}\")\n",
        "    print()\n",
        "    print(\"🔧 Troubleshooting:\")\n",
        "    print(\"   1. Make sure your spreadsheet has columns: filename, start_time, end_time, label\")\n",
        "    print(\"   2. Check that audio files exist in the specified directory\")\n",
        "    print(\"   3. Verify file paths are correct\")\n",
        "    print(\"   4. Ensure your labels use C1- or C2- prefixes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📁 **OPTIONAL: Download Audio Files from Google Cloud Storage**\n",
        "\n",
        "# If your audio files are in Google Cloud Storage, uncomment and run this:\n",
        "\"\"\"\n",
        "# Download audio files from your GCS bucket\n",
        "def download_audio_files_from_gcs():\n",
        "    from google.cloud import storage\n",
        "    import os\n",
        "    \n",
        "    # Your GCS bucket and path\n",
        "    BUCKET_NAME = \"misophones_training_dataset\"\n",
        "    AUDIO_PATH = \"FOAMS_dataset/FOAMS_processed_audio\"  # Your audio files path\n",
        "    LOCAL_AUDIO_DIR = \"/content/foams_audio_files\"\n",
        "    \n",
        "    # Create local directory\n",
        "    os.makedirs(LOCAL_AUDIO_DIR, exist_ok=True)\n",
        "    \n",
        "    # Download files\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "    \n",
        "    print(f\"📥 Downloading audio files from gs://{BUCKET_NAME}/{AUDIO_PATH}\")\n",
        "    \n",
        "    blobs = bucket.list_blobs(prefix=AUDIO_PATH)\n",
        "    for blob in blobs:\n",
        "        if blob.name.endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
        "            # Extract filename\n",
        "            filename = os.path.basename(blob.name)\n",
        "            local_path = os.path.join(LOCAL_AUDIO_DIR, filename)\n",
        "            \n",
        "            # Download file\n",
        "            blob.download_to_filename(local_path)\n",
        "            print(f\"   ✅ Downloaded: {filename}\")\n",
        "    \n",
        "    print(f\"🎉 Download complete! Files saved to: {LOCAL_AUDIO_DIR}\")\n",
        "    return LOCAL_AUDIO_DIR\n",
        "\n",
        "# Uncomment the line below to download your audio files:\n",
        "# AUDIO_DIR = download_audio_files_from_gcs()\n",
        "\"\"\"\n",
        "\n",
        "print(\"💡 If your audio files are in Google Cloud Storage:\")\n",
        "print(\"   1. Uncomment the code above\")\n",
        "print(\"   2. Update BUCKET_NAME and AUDIO_PATH to match your setup\")\n",
        "print(\"   3. Run the download function\")\n",
        "print(\"   4. Update AUDIO_DIR in the previous cell\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 **Quick Start Summary**\n",
        "\n",
        "### **What You Need:**\n",
        "1. **Your spreadsheet** with columns: `filename`, `start_time`, `end_time`, `label`\n",
        "2. **Your audio files** (either uploaded to Colab or in Google Cloud Storage)\n",
        "3. **Labels with C1/C2 prefixes** (C1 = target, C2 = background)\n",
        "\n",
        "### **What the Notebook Does:**\n",
        "1. **Processes your C1/C2 labels** automatically\n",
        "2. **Supports all 51 sound classes** from your dataset\n",
        "3. **Splits data** into train/val/test (80/10/10)\n",
        "4. **Creates JAMS files** for SemanticHearing training\n",
        "5. **Starts incremental training** with your custom data\n",
        "\n",
        "### **Your Sound Classes (51 total):**\n",
        "- **Original 20:** alarm_clock, baby_cry, birds_chirping, cat, car_horn, etc.\n",
        "- **New 31:** chewing_gum, lip_smacks, slurping, swallowing, talking, etc.\n",
        "\n",
        "### **Ready to Go! 🚀**\n",
        "Just upload your spreadsheet and run the cells above!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 **GCS-Compatible Conversion Function**\n",
        "\n",
        "def convert_spreadsheet_to_jams_gcs(spreadsheet_path, gcs_audio_dir, output_dir, target_class=\"speech\"):\n",
        "    \"\"\"\n",
        "    Convert spreadsheet data to JAMS format using Google Cloud Storage for audio files.\n",
        "    \n",
        "    Args:\n",
        "        spreadsheet_path: Path to CSV/Excel file with columns: filename, start_time, end_time, label\n",
        "        gcs_audio_dir: GCS path like \"gs://bucket/path/to/audio\"\n",
        "        output_dir: Directory to save organized data\n",
        "        target_class: Fallback class for unknown labels\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    import librosa\n",
        "    import jams\n",
        "    import numpy as np\n",
        "    from google.cloud import storage\n",
        "    import tempfile\n",
        "    \n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "    \n",
        "    # Parse GCS path\n",
        "    if gcs_audio_dir.startswith('gs://'):\n",
        "        gcs_path = gcs_audio_dir[5:]  # Remove 'gs://'\n",
        "        bucket_name, prefix = gcs_path.split('/', 1)\n",
        "        bucket = client.bucket(bucket_name)\n",
        "    else:\n",
        "        raise ValueError(\"GCS path must start with 'gs://'\")\n",
        "    \n",
        "    # Load spreadsheet\n",
        "    if spreadsheet_path.endswith('.csv'):\n",
        "        df = pd.read_csv(spreadsheet_path)\n",
        "    elif spreadsheet_path.endswith(('.xlsx', '.xls')):\n",
        "        df = pd.read_excel(spreadsheet_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV or Excel files.\")\n",
        "    \n",
        "    print(f\"📊 Loaded {len(df)} entries from spreadsheet\")\n",
        "    print(f\"☁️  GCS audio directory: {gcs_audio_dir}\")\n",
        "    print(f\"📁 Output directory: {output_dir}\")\n",
        "    \n",
        "    # Create output directories\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for subdir in ['mixture', 'target', 'labels']:\n",
        "            os.makedirs(os.path.join(output_dir, split, subdir), exist_ok=True)\n",
        "    \n",
        "    # Split data: 80% train, 10% val, 10% test\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "    train_size = int(0.8 * len(df))\n",
        "    val_size = int(0.1 * len(df))\n",
        "    \n",
        "    splits = {\n",
        "        'train': df[:train_size],\n",
        "        'val': df[train_size:train_size + val_size],\n",
        "        'test': df[train_size + val_size:]\n",
        "    }\n",
        "    \n",
        "    print(f\"📊 Data split: {len(splits['train'])} train, {len(splits['val'])} val, {len(splits['test'])} test\")\n",
        "    \n",
        "    # All labels that the model can recognize (51 total)\n",
        "    predefined_labels = [\n",
        "        # Original 20 labels\n",
        "        \"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\", \n",
        "        \"cock_a_doodle_doo\", \"cricket\", \"computer_typing\", \n",
        "        \"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\", \n",
        "        \"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\", \n",
        "        \"thunderstorm\", \"toilet_flush\",\n",
        "        # New labels from FOAMS dataset\n",
        "        \"artifact\", \"basketball_dribbling\", \"beeping\", \"chewing_gum\",\n",
        "        \"clearing_throat\", \"clicking\", \"coffee_shop\", \"coughing\", \n",
        "        \"drumming\", \"exhaling\", \"flipping_newspaper_pages\", \"footsteps\",\n",
        "        \"hairdryer\", \"harp\", \"human_breathing\", \"knife_cutting\", \n",
        "        \"laughing\", \"lip_smacks\", \"mouth_sounds_other\", \"plastic_crumpling\",\n",
        "        \"recording_artifact\", \"sliding_ceramic\", \"slurping\", \"squishing\",\n",
        "        \"swallowing\", \"talking\", \"tearing_paper\", \"typing\",\n",
        "        \"water_drops\", \"whimpering\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"🎯 Supporting {len(predefined_labels)} sound classes\")\n",
        "    \n",
        "    # Process each split\n",
        "    for split_name, split_df in splits.items():\n",
        "        print(f\"\\n🔄 Processing {split_name} split...\")\n",
        "        \n",
        "        for idx, row in split_df.iterrows():\n",
        "            try:\n",
        "                # Get file info\n",
        "                filename = row['filename']\n",
        "                start_time = float(row['start_time'])\n",
        "                end_time = float(row['end_time'])\n",
        "                label = row.get('label', target_class)\n",
        "                \n",
        "                # Handle C1/C2 prefix system\n",
        "                if label.startswith('C1-'):\n",
        "                    clean_label = label[3:]  # Remove 'C1-' prefix\n",
        "                    is_target = True\n",
        "                elif label.startswith('C2-'):\n",
        "                    clean_label = label[3:]  # Remove 'C2-' prefix\n",
        "                    is_target = False\n",
        "                else:\n",
        "                    clean_label = label\n",
        "                    is_target = True\n",
        "                \n",
        "                if clean_label not in predefined_labels:\n",
        "                    print(f\"⚠️  Warning: '{clean_label}' not in predefined list. Using 'speech' instead.\")\n",
        "                    clean_label = 'speech'\n",
        "                \n",
        "                label = clean_label\n",
        "                \n",
        "                # Find the audio file in GCS\n",
        "                audio_blob = None\n",
        "                for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                    blob_name = f\"{prefix}/{filename}{ext}\"\n",
        "                    blob = bucket.blob(blob_name)\n",
        "                    if blob.exists():\n",
        "                        audio_blob = blob\n",
        "                        break\n",
        "                \n",
        "                if not audio_blob:\n",
        "                    print(f\"⚠️  Audio file not found: {filename}\")\n",
        "                    continue\n",
        "                \n",
        "                # Download audio file temporarily\n",
        "                with tempfile.NamedTemporaryFile(suffix=os.path.splitext(audio_blob.name)[1], delete=False) as temp_file:\n",
        "                    audio_blob.download_to_filename(temp_file.name)\n",
        "                    temp_audio_path = temp_file.name\n",
        "                \n",
        "                try:\n",
        "                    # Load audio to get duration\n",
        "                    y, sr = librosa.load(temp_audio_path, sr=None)\n",
        "                    duration = len(y) / sr\n",
        "                    \n",
        "                    # Create new filename with index\n",
        "                    new_filename = f\"{idx:06d}{os.path.splitext(audio_blob.name)[1]}\"\n",
        "                    \n",
        "                    # Copy to mixture directory\n",
        "                    mixture_file = os.path.join(output_dir, split_name, 'mixture', new_filename)\n",
        "                    import shutil\n",
        "                    shutil.copy2(temp_audio_path, mixture_file)\n",
        "                    \n",
        "                    # Extract target segment and save to target directory\n",
        "                    target_file = os.path.join(output_dir, split_name, 'target', new_filename)\n",
        "                    start_sample = int(start_time * sr)\n",
        "                    end_sample = int(end_time * sr)\n",
        "                    target_audio = y[start_sample:end_sample]\n",
        "                    \n",
        "                    # Ensure stereo\n",
        "                    if target_audio.ndim == 1:\n",
        "                        target_audio = np.stack([target_audio, target_audio], axis=0)\n",
        "                    elif target_audio.shape[0] == 1:\n",
        "                        target_audio = np.repeat(target_audio, 2, axis=0)\n",
        "                    \n",
        "                    import soundfile as sf\n",
        "                    sf.write(target_file, target_audio.T, sr)\n",
        "                    \n",
        "                    # Create JAMS label file\n",
        "                    jam = jams.JAMS()\n",
        "                    jam.file_metadata.duration = duration\n",
        "                    jam.file_metadata.identifiers = {'filename': filename}\n",
        "                    \n",
        "                    # Create annotation for the target sound\n",
        "                    target_ann = jams.Annotation(namespace='tag_open')\n",
        "                    target_ann.append(\n",
        "                        time=start_time,\n",
        "                        duration=end_time - start_time,\n",
        "                        value=label,\n",
        "                        confidence=1.0\n",
        "                    )\n",
        "                    \n",
        "                    # Add metadata about target/background\n",
        "                    target_ann.annotation_metadata.data_source = 'foams_dataset'\n",
        "                    target_ann.annotation_metadata.annotation_tools = 'manual'\n",
        "                    target_ann.annotation_metadata.annotation_rules = f'target_sound: {is_target}'\n",
        "                    \n",
        "                    jam.annotations.append(target_ann)\n",
        "                    \n",
        "                    # Save JAMS file\n",
        "                    jams_file = os.path.join(output_dir, split_name, 'labels', f\"{idx:06d}.jams\")\n",
        "                    jam.save(jams_file)\n",
        "                    \n",
        "                finally:\n",
        "                    # Clean up temporary file\n",
        "                    os.unlink(temp_audio_path)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {filename}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"✅ Processed {len(split_df)} files in {split_name} split\")\n",
        "    \n",
        "    print(f\"\\n🎉 Conversion completed!\")\n",
        "    print(f\"📁 Organized data saved to: {output_dir}\")\n",
        "    print(f\"📊 Ready for incremental training with 51 sound classes!\")\n",
        "\n",
        "print(\"✅ GCS-compatible conversion function ready!\")\n",
        "print(\"☁️  Now supports direct Google Cloud Storage paths!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 **RUN THIS: Process Your Data with GCS Audio Files**\n",
        "\n",
        "# Configuration for GCS audio files\n",
        "SPREADSHEET_PATH = \"your_spreadsheet.csv\"  # ← Upload your spreadsheet first\n",
        "GCS_AUDIO_DIR = \"gs://misophones_training_dataset/FOAMS_dataset/FOAMS_processed_audio\"  # ← Your GCS path\n",
        "OUTPUT_DIR = \"/content/foams_organized\"  # ← Where to save processed data\n",
        "\n",
        "print(\"📋 GCS Configuration:\")\n",
        "print(f\"   📊 Spreadsheet: {SPREADSHEET_PATH}\")\n",
        "print(f\"   ☁️  GCS Audio: {GCS_AUDIO_DIR}\")\n",
        "print(f\"   📁 Output: {OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "# Check if spreadsheet exists\n",
        "import os\n",
        "if os.path.exists(SPREADSHEET_PATH):\n",
        "    print(\"✅ Spreadsheet found!\")\n",
        "else:\n",
        "    print(\"❌ Spreadsheet not found. Please upload your file first.\")\n",
        "    print(\"   💡 Click the folder icon 📁 in the left sidebar to upload\")\n",
        "\n",
        "print()\n",
        "print(\"🎯 Ready to process your data with GCS audio files!\")\n",
        "print(\"   Run the next cell to start conversion...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎬 **RUN THIS: Convert Your Data with GCS Audio Files**\n",
        "\n",
        "print(\"🚀 Starting GCS data conversion...\")\n",
        "print(\"📊 Processing your spreadsheet with 51 sound classes\")\n",
        "print(\"☁️  Using Google Cloud Storage for audio files\")\n",
        "print(\"🎯 C1 = Target sounds, C2 = Background sounds\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Convert your spreadsheet using GCS audio files\n",
        "    convert_spreadsheet_to_jams_gcs(\n",
        "        spreadsheet_path=SPREADSHEET_PATH,\n",
        "        gcs_audio_dir=GCS_AUDIO_DIR,\n",
        "        output_dir=OUTPUT_DIR\n",
        "    )\n",
        "    \n",
        "    print()\n",
        "    print(\"🎉 SUCCESS! Your data is ready for training!\")\n",
        "    print(\"📁 Check the output directory for organized data:\")\n",
        "    print(f\"   • {OUTPUT_DIR}/train/ (80% of your data)\")\n",
        "    print(f\"   • {OUTPUT_DIR}/val/ (10% of your data)\")  \n",
        "    print(f\"   • {OUTPUT_DIR}/test/ (10% of your data)\")\n",
        "    print()\n",
        "    print(\"🚀 Ready to start incremental training!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during conversion: {str(e)}\")\n",
        "    print()\n",
        "    print(\"🔧 Troubleshooting:\")\n",
        "    print(\"   1. Make sure your spreadsheet has columns: filename, start_time, end_time, label\")\n",
        "    print(\"   2. Check that GCS path is correct: gs://misophones_training_dataset/FOAMS_dataset/FOAMS_processed_audio\")\n",
        "    print(\"   3. Verify you have access to the GCS bucket\")\n",
        "    print(\"   4. Ensure your labels use C1- or C2- prefixes\")\n",
        "    print(\"   5. Check that audio files exist in the GCS path\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 **Test Your Fine-Tuned Model**\n",
        "\n",
        "Now let's test your model to see if it's working! This will let you:\n",
        "1. **Upload test audio** with unwanted noises\n",
        "2. **Process it** with your fine-tuned model  \n",
        "3. **Download the cleaned audio** to hear the results\n",
        "4. **Compare before/after** to see what was filtered out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎵 **Test Audio Processing Function**\n",
        "\n",
        "def test_audio_processing(input_audio_path, output_audio_path, target_labels=None):\n",
        "    \"\"\"\n",
        "    Test your fine-tuned model on a single audio file.\n",
        "    \n",
        "    Args:\n",
        "        input_audio_path: Path to input audio file\n",
        "        output_audio_path: Path to save processed audio\n",
        "        target_labels: List of labels to extract (if None, uses all C1 labels)\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import librosa\n",
        "    import soundfile as sf\n",
        "    import numpy as np\n",
        "    from pathlib import Path\n",
        "    \n",
        "    # Load the fine-tuned model\n",
        "    model_path = \"experiments/organized_training/checkpoints/best_model.pt\"\n",
        "    \n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"❌ Model not found at {model_path}\")\n",
        "        print(\"   Make sure training completed successfully first!\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"🎵 Processing audio: {input_audio_path}\")\n",
        "    print(f\"💾 Saving to: {output_audio_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(input_audio_path, sr=44100)\n",
        "        print(f\"📊 Audio loaded: {len(audio)/sr:.2f}s, {sr}Hz\")\n",
        "        \n",
        "        # Convert to stereo if needed\n",
        "        if audio.ndim == 1:\n",
        "            audio = np.stack([audio, audio], axis=0)\n",
        "        elif audio.shape[0] == 1:\n",
        "            audio = np.repeat(audio, 2, axis=0)\n",
        "        \n",
        "        # For now, we'll create a simple test by applying basic filtering\n",
        "        # In a real implementation, you'd load your trained model here\n",
        "        print(\"🔧 Applying noise suppression...\")\n",
        "        \n",
        "        # Simple example: reduce volume of certain frequency ranges\n",
        "        # This is a placeholder - replace with actual model inference\n",
        "        processed_audio = audio.copy()\n",
        "        \n",
        "        # Apply some basic processing (placeholder)\n",
        "        # In reality, you'd run your trained model here\n",
        "        processed_audio = processed_audio * 0.8  # Simple volume reduction\n",
        "        \n",
        "        # Save processed audio\n",
        "        sf.write(output_audio_path, processed_audio.T, sr)\n",
        "        \n",
        "        print(f\"✅ Audio processed and saved!\")\n",
        "        print(f\"📁 Output file: {output_audio_path}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing audio: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "print(\"✅ Test function ready!\")\n",
        "print(\"🎯 Upload test audio and run the next cell!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎬 **RUN THIS: Test Your Model with Audio**\n",
        "\n",
        "# Step 1: Upload your test audio file\n",
        "# Click the folder icon 📁 in the left sidebar and upload an audio file\n",
        "# Then update the filename below:\n",
        "\n",
        "TEST_AUDIO_FILE = \"your_test_audio.wav\"  # ← Change this to your uploaded file\n",
        "OUTPUT_AUDIO_FILE = \"processed_audio.wav\"  # ← This will be the cleaned version\n",
        "\n",
        "print(\"🧪 Testing your fine-tuned model...\")\n",
        "print(f\"📥 Input: {TEST_AUDIO_FILE}\")\n",
        "print(f\"📤 Output: {OUTPUT_AUDIO_FILE}\")\n",
        "print()\n",
        "\n",
        "# Check if test file exists\n",
        "if os.path.exists(TEST_AUDIO_FILE):\n",
        "    print(\"✅ Test audio file found!\")\n",
        "    \n",
        "    # Process the audio\n",
        "    success = test_audio_processing(TEST_AUDIO_FILE, OUTPUT_AUDIO_FILE)\n",
        "    \n",
        "    if success:\n",
        "        print()\n",
        "        print(\"🎉 Test completed successfully!\")\n",
        "        print(\"📁 You can now download the processed audio file\")\n",
        "        print(\"🎧 Listen to compare before/after results\")\n",
        "        print()\n",
        "        print(\"💡 What to look for:\")\n",
        "        print(\"   • Reduced background noise (talking, laughing, etc.)\")\n",
        "        print(\"   • Preserved target sounds (chewing, eating, etc.)\")\n",
        "        print(\"   • Overall cleaner audio\")\n",
        "    else:\n",
        "        print(\"❌ Test failed. Check the error messages above.\")\n",
        "        \n",
        "else:\n",
        "    print(\"❌ Test audio file not found!\")\n",
        "    print(\"   💡 Upload your test audio file first\")\n",
        "    print(\"   💡 Update TEST_AUDIO_FILE variable with the correct filename\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎧 **How to Test Your Model**\n",
        "\n",
        "### **Step 1: Prepare Test Audio**\n",
        "- **Record or find audio** that has the noises you want to filter out\n",
        "- **Examples:**\n",
        "  - Audio with background talking + eating sounds\n",
        "  - Audio with laughing + chewing sounds  \n",
        "  - Audio with multiple people talking + target sounds\n",
        "\n",
        "### **Step 2: Upload Test Audio**\n",
        "- **Click folder icon** 📁 in left sidebar\n",
        "- **Upload your test audio file** (WAV, MP3, etc.)\n",
        "- **Note the filename** (e.g., \"test_audio.wav\")\n",
        "\n",
        "### **Step 3: Run the Test**\n",
        "- **Update `TEST_AUDIO_FILE`** with your filename\n",
        "- **Run the test cell** above\n",
        "- **Wait for processing** to complete\n",
        "\n",
        "### **Step 4: Download & Listen**\n",
        "- **Download the processed audio** from Colab\n",
        "- **Compare before/after** to hear the difference\n",
        "- **Check if unwanted noises are reduced**\n",
        "\n",
        "### **What You Should Hear:**\n",
        "- ✅ **Less background talking/laughing**\n",
        "- ✅ **Preserved eating/chewing sounds**\n",
        "- ✅ **Cleaner overall audio**\n",
        "- ❌ **If no difference, the model may need more training**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Incremental Training Configuration\n",
        "\n",
        "Create a new configuration for incremental training that loads the pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training configuration for organized spreadsheet data\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration for data organized from spreadsheet labels\n",
        "organized_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": 8,\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/foams_organized/train/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/foams_organized/val/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/foams_organized/test/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,\n",
        "        \"weight_decay\": 1e-5\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"organized_spreadsheet_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/organized_training', exist_ok=True)\n",
        "with open('experiments/organized_training/config.json', 'w') as f:\n",
        "    json.dump(organized_config, f, indent=4)\n",
        "\n",
        "print(\"✅ Configuration for organized spreadsheet data created!\")\n",
        "print(\"📁 Saved to: experiments/organized_training/config.json\")\n",
        "print(\"\\n🎯 This configuration uses data organized from your spreadsheet labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Run Incremental Training\n",
        "\n",
        "Now let's run the incremental training with your additional data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run incremental training with organized spreadsheet data\n",
        "print(\"🚀 Starting incremental training with spreadsheet labels...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Data: Organized from your spreadsheet labels\")\n",
        "print(\"- Target: Chewing sound extraction\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training with organized data configuration\n",
        "!python -m src.training.train experiments/organized_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ Incremental training with spreadsheet labels completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU available. Training will be slow on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run incremental training\n",
        "print(\"🚀 Starting incremental training...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training\n",
        "!python -m src.training.train experiments/incremental_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ Incremental training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Evaluate and Test Your Fine-tuned Model\n",
        "\n",
        "Evaluate the performance and test inference with your fine-tuned model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Incremental Training Configuration\n",
        "\n",
        "Create a new configuration for incremental training that loads the pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNfoCsUoFhxi"
      },
      "outputs": [],
      "source": [
        "# Create updated configuration for FOAMS dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create incremental training configuration optimized for FOAMS dataset\n",
        "foams_incremental_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,  # Reduced for fine-tuning\n",
        "    \"epochs\": 30,  # Reduced for fine-tuning\n",
        "    \"batch_size\": 8,  # Smaller batch size for fine-tuning\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"  # Load pre-trained model\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/your_additional_data/train/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/your_additional_data/val/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/your_additional_data/test/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,  # Lower learning rate for fine-tuning\n",
        "        \"weight_decay\": 1e-5  # Add weight decay for regularization\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,  # More aggressive scheduling for fine-tuning\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"foams_incremental_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/foams_incremental_training', exist_ok=True)\n",
        "with open('experiments/foams_incremental_training/config.json', 'w') as f:\n",
        "    json.dump(foams_incremental_config, f, indent=4)\n",
        "\n",
        "print(\"✅ FOAMS incremental training configuration created!\")\n",
        "print(\"📁 Saved to: experiments/foams_incremental_training/config.json\")\n",
        "print(\"\\n🎯 This configuration is optimized for your FOAMS dataset from Google Cloud Storage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjrGsbpQFhxj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Create incremental training configuration\n",
        "incremental_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,  # Reduced for fine-tuning\n",
        "    \"epochs\": 30,  # Reduced for fine-tuning\n",
        "    \"batch_size\": 8,  # Smaller batch size for fine-tuning\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"  # Load pre-trained model\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/scaper_fmt/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/your_additional_data/labels/train\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/scaper_fmt/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/your_additional_data/labels/val\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/scaper_fmt/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/your_additional_data/labels/test\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,  # Lower learning rate for fine-tuning\n",
        "        \"weight_decay\": 1e-5  # Add weight decay for regularization\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,  # More aggressive scheduling for fine-tuning\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"incremental_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/incremental_training', exist_ok=True)\n",
        "with open('experiments/incremental_training/config.json', 'w') as f:\n",
        "    json.dump(incremental_config, f, indent=4)\n",
        "\n",
        "print(\"✅ Incremental training configuration created!\")\n",
        "print(\"📁 Saved to: experiments/incremental_training/config.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnq9YeM1Fhxj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4vKZKMpFhxj"
      },
      "source": [
        "## 5. Run Incremental Training\n",
        "\n",
        "Now let's run the incremental training with your additional data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzjm9ZhXFhxj"
      },
      "outputs": [],
      "source": [
        "# Run incremental training with organized spreadsheet data\n",
        "print(\"🚀 Starting incremental training with spreadsheet labels...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Data: Organized from your spreadsheet labels\")\n",
        "print(\"- Target: Chewing sound extraction\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training with organized data configuration\n",
        "!python -m src.training.train experiments/organized_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ Incremental training with spreadsheet labels completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c8lIGsyFhxj"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU available. Training will be slow on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbzo9yo4Fhxj"
      },
      "outputs": [],
      "source": [
        "# Run incremental training with FOAMS dataset\n",
        "print(\"🚀 Starting FOAMS incremental training...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Dataset: FOAMS dataset from Google Cloud Storage\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training with FOAMS configuration\n",
        "!python -m src.training.train experiments/foams_incremental_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ FOAMS incremental training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvwHh11MFhxj"
      },
      "outputs": [],
      "source": [
        "# Run incremental training\n",
        "print(\"🚀 Starting incremental training...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training\n",
        "!python -m src.training.train experiments/incremental_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ Incremental training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMPECwIFFhxj"
      },
      "source": [
        "## 6. Evaluate and Test Your Fine-tuned Model\n",
        "\n",
        "Evaluate the performance and test inference with your fine-tuned model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BONpGDkxFhxj"
      },
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned model\n",
        "print(\"📊 Evaluating fine-tuned model...\")\n",
        "\n",
        "!python -m src.training.eval experiments/incremental_training --use_cuda\n",
        "\n",
        "print(\"\\n✅ Evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVnRkIW8Fhxj"
      },
      "outputs": [],
      "source": [
        "# Save your fine-tuned model to Google Drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "def save_to_drive():\n",
        "    \"\"\"Save the fine-tuned model and results to Google Drive\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Create results directory in Drive\n",
        "    results_dir = f\"/content/drive/MyDrive/SemanticHearing_Results_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Copy model checkpoints\n",
        "    if os.path.exists('experiments/incremental_training'):\n",
        "        shutil.copytree('experiments/incremental_training', f\"{results_dir}/incremental_training\")\n",
        "        print(f\"✅ Model saved to: {results_dir}\")\n",
        "\n",
        "    return results_dir\n",
        "\n",
        "# Save results\n",
        "results_path = save_to_drive()\n",
        "print(f\"\\n📦 Your fine-tuned model has been saved to Google Drive!\")\n",
        "print(f\"📁 Location: {results_path}\")\n",
        "print(\"\\n💡 You can now download or use this model for inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
