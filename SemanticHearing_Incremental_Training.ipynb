{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SemanticHearing Incremental Training\n",
        "\n",
        "This notebook demonstrates how to perform incremental training on the SemanticHearing model for binaural target sound extraction.\n",
        "\n",
        "## Overview\n",
        "- Load pre-trained model from the original paper\n",
        "- Prepare your additional training data\n",
        "- Fine-tune the model with new data\n",
        "- Evaluate performance improvements\n",
        "\n",
        "**Original Paper**: [Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables](https://dl.acm.org/doi/10.1145/3586183.3606779)\n",
        "\n",
        "**Repository**: https://github.com/sarahv03/SemanticHearing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's install the required dependencies and clone the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchaudio torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install librosa soundfile scipy matplotlib tqdm numpy pandas\n",
        "!pip install torchmetrics==0.10.0 seaborn ipykernel scaper\n",
        "!pip install transformers openl3 youtube_dl bs4 pyroomacoustics\n",
        "!pip install onnx onnxruntime torch_tb_profiler ffmpegio noisereduce\n",
        "!pip install tensorflow tensorflow-probability\n",
        "\n",
        "# Install additional packages for audio processing\n",
        "!pip install scaper thop==0.1.1.post2209072238\n",
        "!pip install python-sofa==0.2.0\n",
        "\n",
        "print(\"✅ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/sarahv03/SemanticHearing.git\n",
        "%cd SemanticHearing\n",
        "\n",
        "print(\"✅ Repository cloned successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Pre-trained Model and Dataset\n",
        "\n",
        "Download the pre-trained model checkpoint and the original dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p experiments/dc_waveformer\n",
        "!mkdir -p data\n",
        "\n",
        "# Download pre-trained model checkpoint\n",
        "!wget -P experiments/dc_waveformer https://semantichearing.cs.washington.edu/39.pt\n",
        "\n",
        "print(\"✅ Pre-trained model downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and set up Google Cloud Storage access\n",
        "from google.colab import drive, auth\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Cloud Storage client\n",
        "client = storage.Client()\n",
        "\n",
        "# Your bucket and dataset paths\n",
        "BUCKET_NAME = \"misophones_training_dataset\"\n",
        "DATASET_PATH = \"FOAMS_dataset/FOAMS_processed_audio\"\n",
        "BINAURAL_DATASET_PATH = \"BinauralCuratedDataset\"  # Original dataset path in your bucket\n",
        "\n",
        "print(f\"✅ Google Cloud Storage client initialized\")\n",
        "print(f\"📦 Bucket: {BUCKET_NAME}\")\n",
        "print(f\"📁 Additional data path: {DATASET_PATH}\")\n",
        "print(f\"📁 Original dataset path: {BINAURAL_DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download datasets from Google Cloud Storage\n",
        "def download_from_gcs(bucket_name, source_path, local_path):\n",
        "    \"\"\"Download files from Google Cloud Storage to local directory\"\"\"\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=source_path)\n",
        "    \n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "    downloaded_files = 0\n",
        "    \n",
        "    for blob in blobs:\n",
        "        # Skip directories\n",
        "        if blob.name.endswith('/'):\n",
        "            continue\n",
        "            \n",
        "        # Create local file path\n",
        "        local_file_path = os.path.join(local_path, blob.name.replace(source_path + '/', ''))\n",
        "        local_dir = os.path.dirname(local_file_path)\n",
        "        os.makedirs(local_dir, exist_ok=True)\n",
        "        \n",
        "        # Download file\n",
        "        blob.download_to_filename(local_file_path)\n",
        "        downloaded_files += 1\n",
        "        \n",
        "        if downloaded_files % 100 == 0:\n",
        "            print(f\"Downloaded {downloaded_files} files...\")\n",
        "    \n",
        "    print(f\"✅ Downloaded {downloaded_files} files from {source_path}\")\n",
        "    return downloaded_files\n",
        "\n",
        "# Download the original BinauralCuratedDataset\n",
        "print(\"📥 Downloading original BinauralCuratedDataset from Google Cloud Storage...\")\n",
        "download_from_gcs(BUCKET_NAME, BINAURAL_DATASET_PATH, \"data/BinauralCuratedDataset\")\n",
        "\n",
        "# Download your additional FOAMS dataset\n",
        "print(\"📥 Downloading FOAMS dataset from Google Cloud Storage...\")\n",
        "download_from_gcs(BUCKET_NAME, DATASET_PATH, \"data/your_additional_data\")\n",
        "\n",
        "print(\"✅ All datasets downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Your Additional Data\n",
        "\n",
        "This section helps you prepare your additional training data. You'll need to:\n",
        "1. Upload your audio files\n",
        "2. Create proper data structure\n",
        "3. Generate labels for your sounds\n",
        "\n",
        "### Data Structure Requirements\n",
        "Your additional data should follow this structure:\n",
        "```\n",
        "your_data/\n",
        "├── train/\n",
        "│   ├── mixture/          # Mixed audio files\n",
        "│   ├── target/           # Target sound files\n",
        "│   └── labels/           # Label files (.jams format)\n",
        "├── val/\n",
        "│   ├── mixture/\n",
        "│   ├── target/\n",
        "│   └── labels/\n",
        "└── test/\n",
        "    ├── mixture/\n",
        "    ├── target/\n",
        "    └── labels/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore your FOAMS dataset structure\n",
        "def explore_dataset_structure(dataset_path):\n",
        "    \"\"\"Explore the structure of your FOAMS dataset\"\"\"\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    \n",
        "    print(f\"🔍 Exploring dataset structure at: {dataset_path}\")\n",
        "    \n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"❌ Dataset path not found: {dataset_path}\")\n",
        "        return\n",
        "    \n",
        "    # Walk through the directory structure\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        level = root.replace(dataset_path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        \n",
        "        # Show some files in each directory\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "# Explore the downloaded dataset\n",
        "explore_dataset_structure(\"data/your_additional_data\")\n",
        "\n",
        "print(\"\\n📋 Next steps:\")\n",
        "print(\"1. Review the dataset structure above\")\n",
        "print(\"2. Run the data preparation script to organize it for training\")\n",
        "print(\"3. The script will create the proper train/val/test splits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory for your additional data\n",
        "!mkdir -p data/your_additional_data/{train,val,test}/{mixture,target,labels}\n",
        "\n",
        "print(\"📁 Created directory structure for your additional data.\")\n",
        "print(\"\\n📋 Next steps:\")\n",
        "print(\"1. Upload your audio files to the appropriate directories\")\n",
        "print(\"2. Create label files in .jams format\")\n",
        "print(\"3. Run the data preparation script below\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated data preparation function for FOAMS dataset\n",
        "def prepare_foams_data(data_dir, target_class=\"speech\"):\n",
        "    \"\"\"\n",
        "    Prepare your FOAMS dataset for training by creating proper train/val/test splits.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing your FOAMS audio files\n",
        "        target_class: Class name for your target sounds (default: \"speech\")\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "    \n",
        "    data_path = Path(data_dir)\n",
        "    \n",
        "    # Find all audio files recursively\n",
        "    audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']\n",
        "    all_audio_files = []\n",
        "    \n",
        "    for ext in audio_extensions:\n",
        "        all_audio_files.extend(data_path.rglob(ext))\n",
        "    \n",
        "    print(f\"Found {len(all_audio_files)} audio files\")\n",
        "    \n",
        "    # Shuffle and split the data\n",
        "    random.shuffle(all_audio_files)\n",
        "    \n",
        "    # Split: 80% train, 10% val, 10% test\n",
        "    train_size = int(0.8 * len(all_audio_files))\n",
        "    val_size = int(0.1 * len(all_audio_files))\n",
        "    \n",
        "    train_files = all_audio_files[:train_size]\n",
        "    val_files = all_audio_files[train_size:train_size + val_size]\n",
        "    test_files = all_audio_files[train_size + val_size:]\n",
        "    \n",
        "    print(f\"Split: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
        "    \n",
        "    # Process each split\n",
        "    for split_name, files in [('train', train_files), ('val', val_files), ('test', test_files)]:\n",
        "        print(f\"\\nProcessing {split_name} split...\")\n",
        "        \n",
        "        # Create directories\n",
        "        split_dir = data_path / split_name\n",
        "        (split_dir / 'mixture').mkdir(parents=True, exist_ok=True)\n",
        "        (split_dir / 'target').mkdir(parents=True, exist_ok=True)\n",
        "        (split_dir / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        for i, audio_file in enumerate(files):\n",
        "            # Create new filename with index\n",
        "            new_name = f\"{i:06d}{audio_file.suffix}\"\n",
        "            \n",
        "            # Copy to mixture directory\n",
        "            mixture_file = split_dir / 'mixture' / new_name\n",
        "            shutil.copy2(audio_file, mixture_file)\n",
        "            \n",
        "            # Copy to target directory (same file for now)\n",
        "            target_file = split_dir / 'target' / new_name\n",
        "            shutil.copy2(audio_file, target_file)\n",
        "            \n",
        "            # Create JAMS label\n",
        "            label_file = split_dir / 'labels' / f\"{i:06d}.jams\"\n",
        "            jam = create_jams_label(mixture_file, target_class)\n",
        "            jam.save(str(label_file))\n",
        "        \n",
        "        print(f\"✅ Processed {len(files)} files in {split_name} split\")\n",
        "    \n",
        "    print(\"\\n🎉 FOAMS dataset preparation completed!\")\n",
        "    print(\"Your data is now organized in the format expected by the training pipeline.\")\n",
        "\n",
        "# Run the data preparation\n",
        "print(\"🚀 Preparing FOAMS dataset for training...\")\n",
        "prepare_foams_data(\"data/your_additional_data\", target_class=\"speech\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert spreadsheet labels to JAMS format for incremental training\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import jams\n",
        "\n",
        "def convert_spreadsheet_to_jams(spreadsheet_path, audio_dir, output_dir, target_class=\"speech\"):\n",
        "    \"\"\"\n",
        "    Convert spreadsheet with labels to JAMS format for SemanticHearing training.\n",
        "    \n",
        "    Required spreadsheet columns:\n",
        "    - filename: Name of the audio file\n",
        "    - start_time: Start time of the target sound (seconds)\n",
        "    - end_time: End time of the target sound (seconds)\n",
        "    - label: Target sound class (must be in predefined list)\n",
        "    \n",
        "    Predefined sound classes:\n",
        "    \"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\", \n",
        "    \"cock_a_doodle_doo\", \"cricket\", \"computer_typing\", \n",
        "    \"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\", \n",
        "    \"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\", \n",
        "    \"thunderstorm\", \"toilet_flush\"\n",
        "    \n",
        "    Args:\n",
        "        spreadsheet_path: Path to your CSV/Excel file\n",
        "        audio_dir: Directory containing the audio files\n",
        "        output_dir: Directory to save organized data\n",
        "        target_class: Default class name if not specified in spreadsheet\n",
        "    \"\"\"\n",
        "    \n",
        "    # Read the spreadsheet\n",
        "    if spreadsheet_path.endswith('.csv'):\n",
        "        df = pd.read_csv(spreadsheet_path)\n",
        "    elif spreadsheet_path.endswith(('.xlsx', '.xls')):\n",
        "        df = pd.read_excel(spreadsheet_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV or Excel files.\")\n",
        "    \n",
        "    print(f\"📊 Loaded {len(df)} entries from spreadsheet\")\n",
        "    print(f\"📁 Audio directory: {audio_dir}\")\n",
        "    print(f\"📁 Output directory: {output_dir}\")\n",
        "    \n",
        "    # Create output directories\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for subdir in ['mixture', 'target', 'labels']:\n",
        "            os.makedirs(os.path.join(output_dir, split, subdir), exist_ok=True)\n",
        "    \n",
        "    # Split data: 80% train, 10% val, 10% test\n",
        "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "    train_size = int(0.8 * len(df))\n",
        "    val_size = int(0.1 * len(df))\n",
        "    \n",
        "    splits = {\n",
        "        'train': df[:train_size],\n",
        "        'val': df[train_size:train_size + val_size],\n",
        "        'test': df[train_size + val_size:]\n",
        "    }\n",
        "    \n",
        "    print(f\"📊 Data split: {len(splits['train'])} train, {len(splits['val'])} val, {len(splits['test'])} test\")\n",
        "    \n",
        "    # Process each split\n",
        "    for split_name, split_df in splits.items():\n",
        "        print(f\"\\n🔄 Processing {split_name} split...\")\n",
        "        \n",
        "        for idx, row in split_df.iterrows():\n",
        "            try:\n",
        "                # Get file info\n",
        "                filename = row['filename']\n",
        "                start_time = float(row['start_time'])\n",
        "                end_time = float(row['end_time'])\n",
        "                label = row.get('label', target_class)\n",
        "                \n",
        "                # Validate label is in predefined list\n",
        "                predefined_labels = [\n",
        "                    \"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\", \n",
        "                    \"cock_a_doodle_doo\", \"cricket\", \"computer_typing\", \n",
        "                    \"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\", \n",
        "                    \"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\", \n",
        "                    \"thunderstorm\", \"toilet_flush\"\n",
        "                ]\n",
        "                \n",
        "                if label not in predefined_labels:\n",
        "                    print(f\"⚠️  Warning: '{label}' not in predefined list. Using '{target_class}' instead.\")\n",
        "                    label = target_class\n",
        "                \n",
        "                # Find the audio file\n",
        "                audio_file = None\n",
        "                for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                    potential_file = os.path.join(audio_dir, filename + ext)\n",
        "                    if os.path.exists(potential_file):\n",
        "                        audio_file = potential_file\n",
        "                        break\n",
        "                \n",
        "                if not audio_file:\n",
        "                    print(f\"⚠️  Audio file not found: {filename}\")\n",
        "                    continue\n",
        "                \n",
        "                # Load audio to get duration\n",
        "                y, sr = librosa.load(audio_file, sr=None)\n",
        "                duration = len(y) / sr\n",
        "                \n",
        "                # Create new filename with index\n",
        "                new_filename = f\"{idx:06d}{os.path.splitext(audio_file)[1]}\"\n",
        "                \n",
        "                # Copy to mixture directory\n",
        "                mixture_file = os.path.join(output_dir, split_name, 'mixture', new_filename)\n",
        "                import shutil\n",
        "                shutil.copy2(audio_file, mixture_file)\n",
        "                \n",
        "                # Extract target segment and save to target directory\n",
        "                target_file = os.path.join(output_dir, split_name, 'target', new_filename)\n",
        "                start_sample = int(start_time * sr)\n",
        "                end_sample = int(end_time * sr)\n",
        "                target_audio = y[start_sample:end_sample]\n",
        "                \n",
        "                # Ensure stereo\n",
        "                if target_audio.ndim == 1:\n",
        "                    target_audio = np.stack([target_audio, target_audio], axis=0)\n",
        "                elif target_audio.shape[0] == 1:\n",
        "                    target_audio = np.repeat(target_audio, 2, axis=0)\n",
        "                \n",
        "                import soundfile as sf\n",
        "                sf.write(target_file, target_audio.T, sr)\n",
        "                \n",
        "                # Create JAMS label file\n",
        "                jam = jams.JAMS()\n",
        "                jam.file_metadata.duration = duration\n",
        "                \n",
        "                # Create annotation for the target sound\n",
        "                target_ann = jams.Annotation(namespace='tag_open')\n",
        "                target_ann.append(\n",
        "                    time=start_time,\n",
        "                    duration=end_time - start_time,\n",
        "                    value=label,\n",
        "                    confidence=1.0\n",
        "                )\n",
        "                jam.annotations.append(target_ann)\n",
        "                \n",
        "                # Save JAMS file\n",
        "                jams_file = os.path.join(output_dir, split_name, 'labels', f\"{idx:06d}.jams\")\n",
        "                jam.save(jams_file)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {filename}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"✅ Processed {len(split_df)} files in {split_name} split\")\n",
        "    \n",
        "    print(f\"\\n🎉 Conversion completed!\")\n",
        "    print(f\"📁 Organized data saved to: {output_dir}\")\n",
        "    print(f\"📊 Ready for incremental training!\")\n",
        "\n",
        "# Example usage - update these paths for your data\n",
        "print(\"📝 Spreadsheet to JAMS conversion script ready!\")\n",
        "print(\"\\nTo use this script:\")\n",
        "print(\"1. Prepare your spreadsheet with columns: filename, start_time, end_time, label\")\n",
        "print(\"2. Update the paths below:\")\n",
        "print(\"3. Run: convert_spreadsheet_to_jams('your_labels.csv', 'audio_directory', 'output_directory')\")\n",
        "print(\"\\nExample:\")\n",
        "print(\"convert_spreadsheet_to_jams('labels.csv', 'data/your_additional_data', 'data/foams_organized', 'chewing')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spreadsheet Format Requirements\n",
        "\n",
        "Based on the SemanticHearing codebase, here's the correct format:\n",
        "\n",
        "### Required Columns:\n",
        "| Column Name | Description | Example |\n",
        "|-------------|-------------|---------|\n",
        "| `filename` | Audio file name (without extension) | `audio_001` |\n",
        "| `start_time` | Start time of target sound (seconds) | `2.3` |\n",
        "| `end_time` | End time of target sound (seconds) | `5.4` |\n",
        "| `label` | Target sound class (must be in predefined list) | `speech` |\n",
        "\n",
        "### Background vs Foreground:\n",
        "- **Foreground (target)**: Sounds you want to extract (must be in the 20 predefined classes)\n",
        "- **Background (non-target)**: Everything else that should be filtered out\n",
        "- **The model determines foreground/background by the label type**, not separate columns\n",
        "\n",
        "### Predefined Sound Classes (from the codebase):\n",
        "```\n",
        "\"alarm_clock\", \"baby_cry\", \"birds_chirping\", \"cat\", \"car_horn\", \n",
        "\"cock_a_doodle_doo\", \"cricket\", \"computer_typing\", \n",
        "\"dog\", \"glass_breaking\", \"gunshot\", \"hammer\", \"music\", \n",
        "\"ocean\", \"door_knock\", \"singing\", \"siren\", \"speech\", \n",
        "\"thunderstorm\", \"toilet_flush\"\n",
        "```\n",
        "\n",
        "### Example Spreadsheet:\n",
        "```csv\n",
        "filename,start_time,end_time,label\n",
        "audio_001,2.3,5.4,speech\n",
        "audio_002,0.5,3.2,music\n",
        "audio_003,1.1,4.7,speech\n",
        "audio_004,2.0,4.0,computer_typing\n",
        "```\n",
        "\n",
        "### For Misophonia (Chewing Sounds):\n",
        "Since \"chewing\" is not in the predefined list, you have two options:\n",
        "1. **Use \"speech\"** as the closest category\n",
        "2. **Add \"chewing\" to the model's label list** (requires code modification)\n",
        "\n",
        "### Supported File Formats:\n",
        "- **CSV files** (`.csv`)\n",
        "- **Excel files** (`.xlsx`, `.xls`)\n",
        "\n",
        "### What the Script Does:\n",
        "1. **Reads your spreadsheet** with timing information\n",
        "2. **Finds corresponding audio files** (supports .wav, .mp3, .flac, .m4a)\n",
        "3. **Extracts target segments** based on start/end times\n",
        "4. **Creates proper directory structure** for training\n",
        "5. **Generates JAMS label files** in the required format\n",
        "6. **Splits data** into train/val/test (80/10/10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create example spreadsheet with background noise information\n",
        "import pandas as pd\n",
        "\n",
        "def create_example_spreadsheet():\n",
        "    \"\"\"Create an example spreadsheet showing the correct format for SemanticHearing\"\"\"\n",
        "    \n",
        "    # Example data using predefined sound classes\n",
        "    data = {\n",
        "        'filename': ['audio_001', 'audio_002', 'audio_003', 'audio_004', 'audio_005'],\n",
        "        'start_time': [2.3, 0.5, 1.1, 3.2, 0.8],\n",
        "        'end_time': [5.4, 3.2, 4.7, 6.1, 2.9],\n",
        "        'label': ['speech', 'speech', 'speech', 'computer_typing', 'speech']\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Save as CSV\n",
        "    df.to_csv('example_labels_with_background.csv', index=False)\n",
        "    \n",
        "    print(\"📊 Example spreadsheet created: example_labels_with_background.csv\")\n",
        "    print(\"\\n📋 This shows the recommended format with background noise information:\")\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create the example\n",
        "example_df = create_example_spreadsheet()\n",
        "\n",
        "print(\"\\n💡 Key points about the correct format:\")\n",
        "print(\"• Use only predefined sound classes from the SemanticHearing model\")\n",
        "print(\"• Foreground vs background is determined by the label type\")\n",
        "print(\"• For misophonia (chewing), use 'speech' as the closest category\")\n",
        "print(\"• The model will learn to extract target sounds and filter everything else\")\n",
        "print(\"\\n🎯 This format helps the model:\")\n",
        "print(\"• Learn to distinguish between target and non-target sounds\")\n",
        "print(\"• Focus on the specific sounds you want to extract\")\n",
        "print(\"• Work with the existing model architecture\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Include Background Noise Information?\n",
        "\n",
        "### **SemanticHearing Model Architecture:**\n",
        "The SemanticHearing model is specifically designed to:\n",
        "- **Extract target sounds** (foreground) from mixed audio\n",
        "- **Suppress background noise** while preserving spatial cues\n",
        "- **Learn the distinction** between what to keep vs. what to filter\n",
        "\n",
        "### **Benefits of Background Noise Labels:**\n",
        "\n",
        "1. **Better Target Extraction** 🎯\n",
        "   - Model learns to focus on specific sounds (chewing)\n",
        "   - Reduces false positives from background noise\n",
        "   - Improves precision in noisy environments\n",
        "\n",
        "2. **Improved Generalization** 🌍\n",
        "   - Trains on diverse acoustic environments\n",
        "   - Learns to handle different background types\n",
        "   - Better performance in real-world scenarios\n",
        "\n",
        "3. **Spatial Audio Preservation** 🎧\n",
        "   - Maintains binaural cues for target sounds\n",
        "   - Filters background while preserving directionality\n",
        "   - Essential for misophonia applications\n",
        "\n",
        "4. **Training Efficiency** ⚡\n",
        "   - Model learns faster with explicit background labels\n",
        "   - Better convergence during incremental training\n",
        "   - More robust to different noise levels\n",
        "\n",
        "### **Background Noise Types to Include:**\n",
        "- **Traffic** (cars, trucks, motorcycles)\n",
        "- **Restaurant** (chatter, dishes, ambient noise)\n",
        "- **Office** (keyboard typing, air conditioning, conversations)\n",
        "- **Music** (background music, radio)\n",
        "- **Nature** (wind, rain, birds)\n",
        "- **Home** (appliances, TV, family sounds)\n",
        "\n",
        "### **Intensity Guidelines:**\n",
        "- **0.0-0.3**: Quiet background (library, quiet room)\n",
        "- **0.3-0.6**: Moderate background (cafe, office)\n",
        "- **0.6-0.8**: Loud background (restaurant, traffic)\n",
        "- **0.8-1.0**: Very loud background (construction, loud music)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Convert your spreadsheet labels to JAMS format\n",
        "# Update these paths to match your data\n",
        "\n",
        "# Path to your spreadsheet (upload to Colab or place in Google Drive)\n",
        "spreadsheet_path = \"labels.csv\"  # or \"labels.xlsx\"\n",
        "\n",
        "# Path to your audio files (downloaded from Google Cloud Storage)\n",
        "audio_directory = \"data/your_additional_data\"\n",
        "\n",
        "# Output directory for organized data\n",
        "output_directory = \"data/foams_organized\"\n",
        "\n",
        "# Target sound class (e.g., \"chewing\", \"speech\", \"music\")\n",
        "target_class = \"chewing\"\n",
        "\n",
        "print(\"📋 Ready to convert your spreadsheet labels!\")\n",
        "print(f\"📊 Spreadsheet: {spreadsheet_path}\")\n",
        "print(f\"🎵 Audio directory: {audio_directory}\")\n",
        "print(f\"📁 Output directory: {output_directory}\")\n",
        "print(f\"🎯 Target class: {target_class}\")\n",
        "\n",
        "# Uncomment the line below to run the conversion\n",
        "# convert_spreadsheet_to_jams(spreadsheet_path, audio_directory, output_directory, target_class)\n",
        "\n",
        "print(\"\\n💡 To run the conversion:\")\n",
        "print(\"1. Upload your spreadsheet to Colab\")\n",
        "print(\"2. Update the paths above\")\n",
        "print(\"3. Uncomment the convert_spreadsheet_to_jams() line\")\n",
        "print(\"4. Run this cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preparation script for your additional data\n",
        "import os\n",
        "import json\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import jams\n",
        "\n",
        "def create_jams_label(audio_file, target_sound_class, start_time=0.0, end_time=None):\n",
        "    \"\"\"\n",
        "    Create a JAMS label file for your audio data.\n",
        "    \n",
        "    Args:\n",
        "        audio_file: Path to audio file\n",
        "        target_sound_class: Class of the target sound (e.g., 'speech', 'music', 'bird')\n",
        "        start_time: Start time of target sound in seconds\n",
        "        end_time: End time of target sound in seconds (None for full duration)\n",
        "    \"\"\"\n",
        "    # Load audio to get duration\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    duration = len(y) / sr\n",
        "    \n",
        "    if end_time is None:\n",
        "        end_time = duration\n",
        "    \n",
        "    # Create JAMS annotation\n",
        "    jam = jams.JAMS()\n",
        "    jam.file_metadata.duration = duration\n",
        "    \n",
        "    # Create annotation for target sound\n",
        "    ann = jams.Annotation(namespace='tag_open')\n",
        "    ann.append(time=start_time, duration=end_time-start_time, value=target_sound_class, confidence=1.0)\n",
        "    \n",
        "    jam.annotations.append(ann)\n",
        "    \n",
        "    return jam\n",
        "\n",
        "def prepare_your_data(data_dir, target_class):\n",
        "    \"\"\"\n",
        "    Prepare your data by creating JAMS labels and organizing files.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing your audio files\n",
        "        target_class: Class name for your target sounds\n",
        "    \"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    \n",
        "    # Process each split\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_dir = data_path / split\n",
        "        if not split_dir.exists():\n",
        "            continue\n",
        "            \n",
        "        print(f\"Processing {split} split...\")\n",
        "        \n",
        "        # Create directories if they don't exist\n",
        "        (split_dir / 'mixture').mkdir(exist_ok=True)\n",
        "        (split_dir / 'target').mkdir(exist_ok=True)\n",
        "        (split_dir / 'labels').mkdir(exist_ok=True)\n",
        "        \n",
        "        # Process audio files\n",
        "        audio_files = list(split_dir.glob('*.wav')) + list(split_dir.glob('*.mp3')) + list(split_dir.glob('*.flac'))\n",
        "        \n",
        "        for audio_file in audio_files:\n",
        "            # Move to mixture directory\n",
        "            mixture_file = split_dir / 'mixture' / audio_file.name\n",
        "            if not mixture_file.exists():\n",
        "                audio_file.rename(mixture_file)\n",
        "            \n",
        "            # Create target file (copy for now - you may want to extract specific parts)\n",
        "            target_file = split_dir / 'target' / audio_file.name\n",
        "            if not target_file.exists():\n",
        "                import shutil\n",
        "                shutil.copy2(mixture_file, target_file)\n",
        "            \n",
        "            # Create JAMS label\n",
        "            label_file = split_dir / 'labels' / (audio_file.stem + '.jams')\n",
        "            if not label_file.exists():\n",
        "                jam = create_jams_label(mixture_file, target_class)\n",
        "                jam.save(str(label_file))\n",
        "        \n",
        "        print(f\"✅ Processed {len(audio_files)} files in {split} split\")\n",
        "\n",
        "# Example usage - modify the paths and target class as needed\n",
        "print(\"📝 Data preparation script ready!\")\n",
        "print(\"\\nTo use this script:\")\n",
        "print(\"1. Upload your audio files to data/your_additional_data/train/, data/your_additional_data/val/, etc.\")\n",
        "print(\"2. Run: prepare_your_data('data/your_additional_data', 'your_target_class')\")\n",
        "print(\"3. Replace 'your_target_class' with the actual class name (e.g., 'speech', 'music', 'bird')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training configuration for organized spreadsheet data\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration for data organized from spreadsheet labels\n",
        "organized_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": 8,\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/foams_organized/train/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/foams_organized/val/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/foams_organized/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/foams_organized/test/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,\n",
        "        \"weight_decay\": 1e-5\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"organized_spreadsheet_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/organized_training', exist_ok=True)\n",
        "with open('experiments/organized_training/config.json', 'w') as f:\n",
        "    json.dump(organized_config, f, indent=4)\n",
        "\n",
        "print(\"✅ Configuration for organized spreadsheet data created!\")\n",
        "print(\"📁 Saved to: experiments/organized_training/config.json\")\n",
        "print(\"\\n🎯 This configuration uses data organized from your spreadsheet labels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Incremental Training Configuration\n",
        "\n",
        "Create a new configuration for incremental training that loads the pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create updated configuration for FOAMS dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create incremental training configuration optimized for FOAMS dataset\n",
        "foams_incremental_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,  # Reduced for fine-tuning\n",
        "    \"epochs\": 30,  # Reduced for fine-tuning\n",
        "    \"batch_size\": 8,  # Smaller batch size for fine-tuning\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"  # Load pre-trained model\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/your_additional_data/train/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/your_additional_data/val/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/your_additional_data/test/labels\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,  # Lower learning rate for fine-tuning\n",
        "        \"weight_decay\": 1e-5  # Add weight decay for regularization\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,  # More aggressive scheduling for fine-tuning\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"foams_incremental_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/foams_incremental_training', exist_ok=True)\n",
        "with open('experiments/foams_incremental_training/config.json', 'w') as f:\n",
        "    json.dump(foams_incremental_config, f, indent=4)\n",
        "\n",
        "print(\"✅ FOAMS incremental training configuration created!\")\n",
        "print(\"📁 Saved to: experiments/foams_incremental_training/config.json\")\n",
        "print(\"\\n🎯 This configuration is optimized for your FOAMS dataset from Google Cloud Storage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Create incremental training configuration\n",
        "incremental_config = {\n",
        "    \"model\": \"src.training.dcc_tf_binaural\",\n",
        "    \"base_metric\": \"scale_invariant_signal_noise_ratio\",\n",
        "    \"fix_lr_epochs\": 10,  # Reduced for fine-tuning\n",
        "    \"epochs\": 30,  # Reduced for fine-tuning\n",
        "    \"batch_size\": 8,  # Smaller batch size for fine-tuning\n",
        "    \"eval_batch_size\": 32,\n",
        "    \"n_workers\": 4,\n",
        "    \"model_params\": {\n",
        "        \"L\": 32,\n",
        "        \"label_len\": 20,\n",
        "        \"model_dim\": 256,\n",
        "        \"num_enc_layers\": 10,\n",
        "        \"num_dec_layers\": 1,\n",
        "        \"dec_buf_len\": 13,\n",
        "        \"dec_chunk_size\": 13,\n",
        "        \"use_pos_enc\": True,\n",
        "        \"conditioning\": \"mult\",\n",
        "        \"out_buf_len\": 4,\n",
        "        \"pretrained_path\": \"experiments/dc_waveformer/39.pt\"  # Load pre-trained model\n",
        "    },\n",
        "    \"train_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"train_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/scaper_fmt/train\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/train\",\n",
        "        \"jams_dir\": \"data/your_additional_data/labels/train\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"train\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"val_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"val_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/scaper_fmt/val\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-development\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/val\",\n",
        "        \"jams_dir\": \"data/your_additional_data/labels/val\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"val\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"test_dataset\": \"src.training.datasets.curated_binaural_augrir.CuratedBinauralAugRIRDataset\",\n",
        "    \"test_data_args\": {\n",
        "        \"fg_dir\": \"data/your_additional_data/scaper_fmt/test\",\n",
        "        \"bg_dir\": \"data/BinauralCuratedDataset/TAU-acoustic-sounds/TAU-urban-acoustic-scenes-2019-evaluation\",\n",
        "        \"bg_scaper_dir\": \"data/BinauralCuratedDataset/bg_scaper_fmt/test\",\n",
        "        \"jams_dir\": \"data/your_additional_data/labels/test\",\n",
        "        \"hrtf_dir\": \"data/BinauralCuratedDataset/hrtf\",\n",
        "        \"dset\": \"test\",\n",
        "        \"sr\": 44100,\n",
        "        \"resample_rate\": None,\n",
        "        \"reverb\": True\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"lr\": 0.0001,  # Lower learning rate for fine-tuning\n",
        "        \"weight_decay\": 1e-5  # Add weight decay for regularization\n",
        "    },\n",
        "    \"lr_sched\": {\n",
        "        \"mode\": \"max\",\n",
        "        \"factor\": 0.5,\n",
        "        \"patience\": 3,  # More aggressive scheduling for fine-tuning\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"threshold\": 0.01,\n",
        "        \"threshold_mode\": \"abs\"\n",
        "    },\n",
        "    \"commit_hash\": \"incremental_training_v1\"\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "os.makedirs('experiments/incremental_training', exist_ok=True)\n",
        "with open('experiments/incremental_training/config.json', 'w') as f:\n",
        "    json.dump(incremental_config, f, indent=4)\n",
        "\n",
        "print(\"✅ Incremental training configuration created!\")\n",
        "print(\"📁 Saved to: experiments/incremental_training/config.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Incremental Training\n",
        "\n",
        "Now let's run the incremental training with your additional data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run incremental training with organized spreadsheet data\n",
        "print(\"🚀 Starting incremental training with spreadsheet labels...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Data: Organized from your spreadsheet labels\")\n",
        "print(\"- Target: Chewing sound extraction\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training with organized data configuration\n",
        "!python -m src.training.train experiments/organized_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ Incremental training with spreadsheet labels completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU available. Training will be slow on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run incremental training with FOAMS dataset\n",
        "print(\"🚀 Starting FOAMS incremental training...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Dataset: FOAMS dataset from Google Cloud Storage\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training with FOAMS configuration\n",
        "!python -m src.training.train experiments/foams_incremental_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ FOAMS incremental training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run incremental training\n",
        "print(\"🚀 Starting incremental training...\")\n",
        "print(\"\\n📋 Training configuration:\")\n",
        "print(\"- Model: Pre-trained SemanticHearing model\")\n",
        "print(\"- Epochs: 30\")\n",
        "print(\"- Learning rate: 0.0001\")\n",
        "print(\"- Batch size: 8\")\n",
        "print(\"\\n⏳ This may take several hours depending on your data size...\")\n",
        "\n",
        "# Run training\n",
        "!python -m src.training.train experiments/incremental_training --use_cuda --start_epoch 0\n",
        "\n",
        "print(\"\\n✅ Incremental training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate and Test Your Fine-tuned Model\n",
        "\n",
        "Evaluate the performance and test inference with your fine-tuned model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned model\n",
        "print(\"📊 Evaluating fine-tuned model...\")\n",
        "\n",
        "!python -m src.training.eval experiments/incremental_training --use_cuda\n",
        "\n",
        "print(\"\\n✅ Evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save your fine-tuned model to Google Drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "def save_to_drive():\n",
        "    \"\"\"Save the fine-tuned model and results to Google Drive\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Create results directory in Drive\n",
        "    results_dir = f\"/content/drive/MyDrive/SemanticHearing_Results_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy model checkpoints\n",
        "    if os.path.exists('experiments/incremental_training'):\n",
        "        shutil.copytree('experiments/incremental_training', f\"{results_dir}/incremental_training\")\n",
        "        print(f\"✅ Model saved to: {results_dir}\")\n",
        "    \n",
        "    return results_dir\n",
        "\n",
        "# Save results\n",
        "results_path = save_to_drive()\n",
        "print(f\"\\n📦 Your fine-tuned model has been saved to Google Drive!\")\n",
        "print(f\"📁 Location: {results_path}\")\n",
        "print(\"\\n💡 You can now download or use this model for inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
